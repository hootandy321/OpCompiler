# ğŸ“‚ `ntops/` ç®—å­åº“æ¶æ„å…¨æ™¯

## 1. å­ç³»ç»ŸèŒè´£

**ntops** (NineToothed OPS) æ˜¯åŸºäº **ninetoothed** ç¼–è¯‘å™¨æ„å»ºçš„é«˜æ€§èƒ½æ·±åº¦å­¦ä¹ ç®—å­åº“ã€‚å®ƒä¸º Infini ç”Ÿæ€ç³»ç»Ÿæä¾›äº†ä¸€ç³»åˆ—ä¼˜åŒ–çš„GPUç®—å­,è¦†ç›–ä»åŸºç¡€æ•°å­¦è¿ç®—åˆ°å¤æ‚çš„Transformeræ ¸å¿ƒç®—å­ã€‚

è¯¥æ¨¡å—åœ¨æ•´ä¸ª Infini æ¶æ„ä¸­çš„ä½œç”¨:
- **ç®—å­å®ç°å±‚**: å°†ninetoothedçš„ç¬¦å·åŒ–ç¼–è¯‘èƒ½åŠ›å°è£…ä¸ºå¯ç”¨çš„ç®—å­API
- **PyTorchå…¼å®¹å±‚**: æä¾›ä¸PyTorch APIå…¼å®¹çš„æ¥å£,ä¾¿äºæ— ç¼é›†æˆ
- **æ€§èƒ½ä¼˜åŒ–å±‚**: é€šè¿‡è‡ªåŠ¨è°ƒä¼˜å’Œå†…å­˜å¸ƒå±€ä¼˜åŒ–æä¾›è¶…è¶ŠåŸç”ŸPyTorchçš„æ€§èƒ½

## 2. æ¨¡å—å¯¼èˆª

### æ ¸å¿ƒå†…æ ¸å®ç° (`src/ntops/kernels/`)

**ç›®å½•ç»“æ„**:
```
kernels/
â”œâ”€â”€ __init__.py              # å†…æ ¸å¯¼å‡ºæ¥å£
â”œâ”€â”€ element_wise.py          # é€å…ƒç´ ç®—å­åŸºç±»
â”œâ”€â”€ reduction.py             # å½’çº¦ç®—å­
â”œâ”€â”€ add.py, mul.py, sub.py... # åŸºç¡€ç®—æœ¯ç®—å­
â”œâ”€â”€ relu.py, gelu.py, silu.py... # æ¿€æ´»å‡½æ•°
â”œâ”€â”€ layer_norm.py, rms_norm.py  # å½’ä¸€åŒ–ç®—å­
â”œâ”€â”€ matmul.py, bmm.py, mm.py    # çŸ©é˜µä¹˜æ³•
â”œâ”€â”€ softmax.py, sigmoid.py...   # æ•°å­¦å‡½æ•°
â”œâ”€â”€ scaled_dot_product_attention.py  # æ³¨æ„åŠ›æœºåˆ¶
â””â”€â”€ rotary_position_embedding.py     # æ—‹è½¬ä½ç½®ç¼–ç 
```

**è®¾è®¡æ¨¡å¼**:
- **ä¸‰æ®µå¼ç»“æ„**: æ¯ä¸ªç®—å­åŒ…å« `arrangement`, `application`, `tensors` ä¸‰éƒ¨åˆ†
- **premakeå‡½æ•°**: è¿”å› `(arrangement_func, application_func, tensors)` ä¸‰å…ƒç»„
- **æ¨¡å—åŒ–**: åŸºç¡€ç®—å­å¯ç»„åˆå½¢æˆå¤æ‚ç®—å­

**æ ¸å¿ƒç»„ä»¶**:

#### 2.1 é€å…ƒç´ ç®—å­ (`element_wise.py`)
- **arrangementå‡½æ•°**: å°†è¾“å…¥å¼ é‡å±•å¹³å¹¶åˆ†å—
  ```python
  def arrangement(*tensors, block_size=None):
      return tuple(
          tensor.flatten().tile((block_size,)) if tensor.ndim != 0 else tensor
          for tensor in tensors
      )
  ```
- **ç‰¹ç‚¹**: é€‚ç”¨äºæ‰€æœ‰é€å…ƒç´ æ“ä½œ(åŠ ã€å‡ã€ä¹˜ã€é™¤ã€æ¿€æ´»å‡½æ•°ç­‰)

#### 2.2 çŸ©é˜µä¹˜æ³• (`mm.py`, `bmm.py`)
- **åˆ†å—ç­–ç•¥**: ä½¿ç”¨å¯é…ç½®çš„ `BLOCK_SIZE_M`, `BLOCK_SIZE_N`, `BLOCK_SIZE_K`
- **ç²¾åº¦å˜ä½“**: æ”¯æŒ IEEE float32 å’Œ TF32 ä¸¤ç§ç²¾åº¦æ¨¡å¼
- **èåˆæ“ä½œ**: `addmm` å®ç°çŸ©é˜µä¹˜æ³•åŠ æ³•èåˆ

#### 2.3 æ³¨æ„åŠ›æœºåˆ¶ (`scaled_dot_product_attention.py`)
- **å¤æ‚å¸ƒå±€**: å¤šçº§åˆ†å—å’Œå¹¿æ’­æ“ä½œ
- **KVç¼“å­˜**: æ”¯æŒå¸¦KVç¼“å­˜çš„æ¨ç†æ¨¡å¼
- **å› æœæ©ç **: æ”¯æŒ `UPPER_LEFT` å’Œ `LOWER_RIGHT` ä¸¤ç§å› æœå˜ä½“
- **å‚æ•°**:
  - `query`, `key`, `value`: æ³¨æ„åŠ›çš„ä¸‰è¦ç´ 
  - `attn_mask`: æ³¨æ„åŠ›æ©ç 
  - `is_causal`: æ˜¯å¦ä½¿ç”¨å› æœæ©ç 
  - `scale`: ç¼©æ”¾å› å­

#### 2.4 å½’ä¸€åŒ–ç®—å­ (`layer_norm.py`, `rms_norm.py`)
- **LayerNorm**: å®Œæ•´çš„å±‚å½’ä¸€åŒ–å®ç°,æ”¯æŒå¯å­¦ä¹ å‚æ•°
- **RMSNorm**: æ›´ç®€åŒ–çš„æ ¹å‡æ–¹å½’ä¸€åŒ–
- **ä¼˜åŒ–ç­–ç•¥**: æ²¿å½’ä¸€åŒ–ç»´åº¦åˆ†å—,å‡å°‘å…¨å±€å†…å­˜è®¿é—®

### PyTorchæ¥å£å°è£… (`src/ntops/torch/`)

**ç›®å½•ç»“æ„**:
```
torch/
â”œâ”€â”€ __init__.py       # PyTorchæ¥å£å¯¼å‡º
â”œâ”€â”€ utils.py          # å·¥å…·å‡½æ•°å’Œç¼“å­˜
â”œâ”€â”€ add.py            # torch.addå…¼å®¹æ¥å£
â”œâ”€â”€ mul.py            # torch.mulå…¼å®¹æ¥å£
â”œâ”€â”€ matmul.py         # torch.matmulå…¼å®¹æ¥å£
â””â”€â”€ ...               # å…¶ä»–ç®—å­çš„PyTorchå°è£…
```

**è®¾è®¡æ¨¡å¼**:

#### ç»Ÿä¸€æ¥å£æ¨¡å¼
æ¯ä¸ªç®—å­éµå¾ªç›¸åŒçš„å°è£…æ¨¡å¼:
```python
def op_name(input, other, *, out=None):
    if out is None:
        out = torch.empty_like(input)

    kernel = _cached_make(ntops.kernels.op_name.premake, input.ndim)
    kernel(input, other, out)

    return out
```

#### å…³é”®ç‰¹æ€§
- **APIå…¼å®¹**: å®Œå…¨å…¼å®¹PyTorchå‡½æ•°ç­¾å
- **è‡ªåŠ¨å†…å­˜ç®¡ç†**: è‡ªåŠ¨åˆ›å»ºè¾“å‡ºå¼ é‡(å¦‚æœæœªæä¾›)
- **å†…æ ¸ç¼“å­˜**: `_cached_make` ç¡®ä¿ç›¸åŒé…ç½®åªç¼–è¯‘ä¸€æ¬¡
- **ç±»å‹ä¿æŒ**: è¾“å‡ºå¼ é‡ä¿æŒè¾“å…¥å¼ é‡çš„è®¾å¤‡å’Œæ•°æ®ç±»å‹

#### å·¥å…·å‡½æ•° (`utils.py`)

**å†…æ ¸ç¼“å­˜æœºåˆ¶**:
```python
@functools.cache
def _cached_make(premake, *args, num_warps, num_stages, max_num_configs, **keywords):
    return ninetoothed.make(
        *premake(*args, **keywords),
        num_warps=num_warps,
        num_stages=num_stages,
        max_num_configs=max_num_configs,
    )
```

**å…¨å±€é…ç½®ç®¡ç†**:
- `set_default_num_warps(num_warps)`: è®¾ç½®é»˜è®¤warpæ•°
- `set_default_num_stages(num_stages)`: è®¾ç½®é»˜è®¤æµæ°´çº¿é˜¶æ®µæ•°
- `set_default_max_num_configs(max_num_configs)`: è®¾ç½®æœ€å¤§è°ƒä¼˜é…ç½®æ•°

**ç²¾åº¦é€‚é…**:
```python
def _get_matmul_input_precision():
    if torch.get_float32_matmul_precision() == "highest":
        return ntops.kernels.mm.InputPrecisionVariant.IEEE
    return ntops.kernels.mm.InputPrecisionVariant.TF32
```

### æµ‹è¯•å¥—ä»¶ (`tests/`)

**æµ‹è¯•è¦†ç›–**:
- **åŸºç¡€ç®—æœ¯**: `test_add.py`, `test_sub.py`, `test_mul.py`, `test_div.py`
- **æ¯”è¾ƒè¿ç®—**: `test_eq.py`, `test_lt.py`, `test_gt.py`, `test_le.py`, `test_ge.py`, `test_ne.py`
- **æ•°å­¦å‡½æ•°**: `test_sin.py`, `test_cos.py`, `test_exp.py`, `test_pow.py`, `test_tanh.py`
- **æ¿€æ´»å‡½æ•°**: `test_relu.py`, `test_gelu.py`, `test_sigmoid.py`
- **å½’ä¸€åŒ–**: `test_layer_norm.py`, `test_rms_norm.py`
- **çŸ©é˜µè¿ç®—**: `test_mm.py`, `test_bmm.py`, `test_addmm.py`, `test_matmul.py`
- **ç‰¹æ®Šå‡½æ•°**: `test_clamp.py`, `test_dropout.py`, `test_softmax.py`
- **é«˜çº§ç®—å­**: `test_scaled_dot_product_attention.py`
- **ä½è¿ç®—**: `test_bitwise_and.py`, `test_bitwise_or.py`, `test_bitwise_not.py`
- **ç‰¹æ®Šå€¼**: `test_isinf.py`, `test_isnan.py`

**æµ‹è¯•å·¥å…·** (`conftest.py`, `skippers.py`, `utils.py`):
- è®¾å¤‡æ£€æµ‹(CPU/CUDA)
- éšæœºç§å­ç®¡ç†
- æ¡ä»¶è·³è¿‡(å¦‚æ— GPUæ—¶è·³è¿‡CUDAæµ‹è¯•)

## 3. æ¶æ„é€»è¾‘å›¾è§£

### 3.1 åŒå±‚æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ç”¨æˆ·ä»£ç (User Code)                         â”‚
â”‚  import ntops.torch as torch_ops                    â”‚
â”‚  result = torch_ops.add(x, y)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       PyTorchæ¥å£å±‚ (torch/)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  APIå°è£…                                    â”‚   â”‚
â”‚  â”‚  â€¢ å…¼å®¹PyTorchå‡½æ•°ç­¾å                      â”‚   â”‚
â”‚  â”‚  â€¢ è‡ªåŠ¨å†…å­˜ç®¡ç†                              â”‚   â”‚
â”‚  â”‚  â€¢ å‚æ•°éªŒè¯                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  å†…æ ¸ç¼“å­˜(_cached_make)                     â”‚   â”‚
â”‚  â”‚  â€¢ functools.cacheç¼“å­˜ç¼–è¯‘ç»“æœ               â”‚   â”‚
â”‚  â”‚  â€¢ å…¨å±€é…ç½®ç®¡ç†                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       å†…æ ¸æŠ½è±¡å±‚ (kernels/)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  premakeå‡½æ•°                                â”‚   â”‚
â”‚  â”‚  è¿”å›: (arrangement, application, tensors)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  arrangementå‡½æ•°                            â”‚   â”‚
â”‚  â”‚  â€¢ å®šä¹‰å†…å­˜å¸ƒå±€(tile/expand/squeeze)         â”‚   â”‚
â”‚  â”‚  â€¢ ä¼˜åŒ–æ•°æ®è®¿é—®æ¨¡å¼                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  applicationå‡½æ•°                            â”‚   â”‚
â”‚  â”‚  â€¢ å®šä¹‰è®¡ç®—é€»è¾‘                              â”‚   â”‚
â”‚  â”‚  â€¢ ä¸å¸ƒå±€æ— å…³çš„æŠ½è±¡ç®—æ³•                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       ninetoothedç¼–è¯‘å±‚                             â”‚
â”‚  ninetoothed.make(arrangement, application, tensors)â”‚
â”‚  â€¢ ç¬¦å·åŒ–å¼ é‡æ“ä½œ                                    â”‚
â”‚  â€¢ ASTä»£ç ç”Ÿæˆ                                      â”‚
â”‚  â€¢ è‡ªåŠ¨è°ƒä¼˜                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       GPUå†…æ ¸æ‰§è¡Œ                                    â”‚
â”‚  Tritonç¼–è¯‘ â†’ PTX â†’ GPUæ‰§è¡Œ                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 ç®—å­å®ç°èŒƒå¼

**æ ‡å‡†ç®—å­æ¨¡æ¿**:
```python
# æ­¥éª¤1: å®šä¹‰application(è®¡ç®—é€»è¾‘)
def application(input, other, output):
    output = input + other  # ç¬¦å·åŒ–æ“ä½œ

# æ­¥éª¤2: å®šä¹‰arrangement(å†…å­˜å¸ƒå±€)
def arrangement(*tensors, block_size=None):
    return tuple(
        tensor.flatten().tile((block_size,))
        for tensor in tensors
    )

# æ­¥éª¤3: å®šä¹‰premake(å·¥å‚å‡½æ•°)
def premake(ndim, dtype=None, block_size=None):
    arrangement_ = functools.partial(arrangement, block_size=block_size)
    tensors = (
        Tensor(ndim, dtype=dtype),
        Tensor(ndim, dtype=dtype),
        Tensor(ndim, dtype=dtype),
    )
    return arrangement_, application, tensors

# æ­¥éª¤4: PyTorchå°è£…
def add(input, other, *, out=None):
    if out is None:
        out = torch.empty_like(input)
    kernel = _cached_make(premake, input.ndim)
    kernel(input, other, out)
    return out
```

### 3.3 å¤æ‚ç®—å­ç¤ºä¾‹:SDPA

**Scaled Dot-Product Attentionçš„å†…å­˜å¸ƒå±€**:

```
è¾“å…¥å¼ é‡:
  query: (batch, seq_q, num_heads, head_dim)
  key:   (batch, seq_k, num_heads, head_dim)
  value: (batch, seq_k, num_heads, head_dim)

æ­¥éª¤1: åˆ†å—
  query_arranged: (batch, num_heads, seq_q, head_dim)
                â†’ (batch, num_heads, seq_q/BLOCK_M, BLOCK_M, head_dim)

  key_arranged:   (batch, num_heads, seq_k, head_dim)
                â†’ (batch, num_heads, seq_k/BLOCK_N, BLOCK_N, head_dim)

  value_arranged: åŒkey

æ­¥éª¤2: å¹¿æ’­å¯¹é½
  query: (batch, num_heads, seq_q/BLOCK_M, BLOCK_M, 1, head_dim)
  key:   (batch, num_heads, 1, BLOCK_N, seq_k/BLOCK_N, head_dim)
  value: (batch, num_heads, 1, BLOCK_N, seq_k/BLOCK_N, head_dim)

æ­¥éª¤3: åº”ç”¨è®¡ç®—
  for m in range(BLOCK_M):
      for n in range(BLOCK_N):
          attn_score = query[m] @ key[n].T  # (head_dim, head_dim)
          attn_weight = softmax(attn_score * scale)
          output[m] += attn_weight @ value[n]
```

## 4. ç®—å­åˆ†ç±»ä¸ç‰¹æ€§

### 4.1 åŸºç¡€ç®—æœ¯ç®—å­
| ç®—å­ | åŠŸèƒ½ | ç‰¹æ®Šå‚æ•° |
|------|------|---------|
| `add` | åŠ æ³• | `alpha`(ç¼©æ”¾å› å­) |
| `sub` | å‡æ³• | - |
| `mul` | ä¹˜æ³• | - |
| `div` | é™¤æ³• | - |
| `addmm` | çŸ©é˜µä¹˜åŠ èåˆ | `beta`(è¾“å‡ºç¼©æ”¾), `alpha`(çŸ©é˜µç¼©æ”¾) |

### 4.2 æ¯”è¾ƒè¿ç®—ç®—å­
| ç®—å­ | åŠŸèƒ½ | è¾“å‡ºç±»å‹ |
|------|------|---------|
| `eq` | ç­‰äº | bool |
| `lt`/`le` | å°äº/å°äºç­‰äº | bool |
| `gt`/`ge` | å¤§äº/å¤§äºç­‰äº | bool |
| `ne` | ä¸ç­‰äº | bool |

### 4.3 æ•°å­¦å‡½æ•°ç®—å­
| ç®—å­ | å‡½æ•° | æ•°å€¼ç¨³å®šæ€§ |
|------|------|-----------|
| `exp` | æŒ‡æ•° | éœ€å¤„ç†æº¢å‡º |
| `sin/cos` | ä¸‰è§’å‡½æ•° | ç›´æ¥æ˜ å°„libdevice |
| `pow` | å¹‚è¿ç®— | æ”¯æŒæ•´æ•°å’Œæµ®ç‚¹æŒ‡æ•° |
| `rsqrt` | å¹³æ–¹æ ¹å€’æ•° | å¸¸ç”¨äºLayerNorm |

### 4.4 æ¿€æ´»å‡½æ•°ç®—å­
| ç®—å­ | å…¬å¼ | è¿‘ä¼¼æ¨¡å¼ |
|------|------|---------|
| `relu` | `max(0, x)` | - |
| `gelu` | `x * Î¦(x)` | æ”¯æŒ`"tanh"`è¿‘ä¼¼ |
| `silu` | `x / (1 + e^(-x))` | - |
| `sigmoid` | `1 / (1 + e^(-x))` | - |
| `tanh` | åŒæ›²æ­£åˆ‡ | ç›´æ¥æ˜ å°„libdevice |

### 4.5 å½’ä¸€åŒ–ç®—å­
| ç®—å­ | å½’ä¸€åŒ–ç»´åº¦ | å¯å­¦ä¹ å‚æ•° |
|------|-----------|-----------|
| `layer_norm` | æœ€åCç»´ | weight, bias |
| `rms_norm` | æœ€åCç»´ | weight |

**LayerNormè®¡ç®—**:
```python
mean = x.mean(dim=-1, keepdim=True)
var = x.var(dim=-1, keepdim=True)
output = (x - mean) / sqrt(var + eps) * weight + bias
```

**RMSNormè®¡ç®—**:
```python
rms = sqrt(mean(x^2, dim=-1, keepdim=True) + eps)
output = (x / rms) * weight
```

### 4.6 çŸ©é˜µè¿ç®—ç®—å­
| ç®—å­ | è¾“å…¥å½¢çŠ¶ | è¾“å‡ºå½¢çŠ¶ |
|------|---------|---------|
| `mm` | (M, K), (K, N) | (M, N) |
| `bmm` | (B, M, K), (B, K, N) | (B, M, N) |
| `matmul` | å¹¿æ’­ | å¹¿æ’­ |

**ä¼˜åŒ–ç‰¹æ€§**:
- **åˆ†å—ç­–ç•¥**: å¯é…ç½®çš„å—å¤§å°(BLOCK_SIZE_M/N/K)
- **ç²¾åº¦æ¨¡å¼**: IEEE float32 æˆ– TF32
- **èåˆ**: `addmm`èåˆçŸ©é˜µä¹˜æ³•å’ŒåŠ æ³•

### 4.7 ç‰¹æ®Šç®—å­

#### Dropout (`dropout.py`)
- **è®­ç»ƒæ¨¡å¼**: éšæœºmask,æŒ‰æ¦‚ç‡ç½®é›¶
- **æ¨ç†æ¨¡å¼**: æ’ç­‰æ˜ å°„
- **å®ç°**: é€šè¿‡ `training` å‚æ•°åˆ‡æ¢

#### Softmax (`softmax.py`)
- **æ•°å€¼ç¨³å®š**: å‡å»æœ€å¤§å€¼é¿å…æº¢å‡º
- **æ”¯æŒå¤šç»´åº¦**: æ²¿æŒ‡å®šç»´åº¦å½’ä¸€åŒ–

#### Clamp (`clamp.py`)
- **æˆªæ–­**: å°†å€¼é™åˆ¶åœ¨[min, max]èŒƒå›´å†…
- **ç”¨é€”**: æ¿€æ´»å‡½æ•°è£å‰ªã€æ¢¯åº¦è£å‰ª

#### SDPA (`scaled_dot_product_attention.py`)
**ç‰¹æ€§**:
- **KVç¼“å­˜**: æ”¯æŒå¢é‡æ¨ç†
- **å› æœæ©ç **: è‡ªå›å½’ç”Ÿæˆ
- **æ³¨æ„åŠ›æ©ç **: çµæ´»çš„paddingå’Œfuture masking

#### RoPE (`rotary_position_embedding.py`)
- **æ—‹è½¬ä½ç½®ç¼–ç **: å¢å¼ºTransformerçš„ä½ç½®æ„ŸçŸ¥
- **èåˆ**: ä¸æ³¨æ„åŠ›è®¡ç®—èåˆ

## 5. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 5.1 å†…å­˜å¸ƒå±€ä¼˜åŒ–
- **åˆ†å—(Tile)**: å°†å¤§å¼ é‡åˆ†è§£ä¸ºå°å—,æé«˜ç¼“å­˜åˆ©ç”¨ç‡
- **å‘é‡åŒ–**: åˆ©ç”¨GPU SIMTæ¶æ„
- **åˆå¹¶è®¿é—®**: ç¡®ä¿å†…å­˜è®¿é—®åˆå¹¶

### 5.2 è®¡ç®—ä¼˜åŒ–
- **å†…æ ¸èåˆ**: å¤šä¸ªæ“ä½œèåˆä¸ºå•ä¸ªå†…æ ¸,å‡å°‘å†…å­˜è®¿é—®
- **æµæ°´çº¿(Pipeline)**: éšè—å†…å­˜å»¶è¿Ÿ
- **è‡ªåŠ¨è°ƒä¼˜**: æ ¹æ®ç¡¬ä»¶è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜é…ç½®

### 5.3 ç¼–è¯‘ä¼˜åŒ–
- **å†…æ ¸ç¼“å­˜**: ç›¸åŒé…ç½®åªç¼–è¯‘ä¸€æ¬¡
- **ç¬¦å·è®¡ç®—**: ç¼–è¯‘æ—¶æ±‚å¸¸é‡
- **å‡½æ•°å†…è”**: å‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€

## 6. ä¸PyTorchçš„å¯¹æ¯”

| ç‰¹æ€§ | PyTorchåŸç”Ÿ | ntops |
|------|------------|-------|
| **æ€§èƒ½** | é€šç”¨ä¼˜åŒ– | é’ˆå¯¹ç‰¹å®šå½¢çŠ¶ä¼˜åŒ– |
| **çµæ´»æ€§** | å®Œå…¨åŠ¨æ€ | éœ€è¦å›ºå®šç»´åº¦(ç¼–è¯‘æ—¶) |
| **è°ƒä¼˜** | æ‰‹åŠ¨è°ƒæ•´ | è‡ªåŠ¨è°ƒä¼˜ |
| **API** | æ ‡å‡†PyTorch API | å®Œå…¨å…¼å®¹ |
| **åç«¯** | ATen/CUDA | Triton |

## 7. ä½¿ç”¨ç¤ºä¾‹

### 7.1 åŸºç¡€ç®—å­
```python
import ntops.torch as torch_ops
import torch

x = torch.randn(1024, device='cuda')
y = torch.randn(1024, device='cuda')

# ä½¿ç”¨ntopsåŠ é€Ÿçš„åŠ æ³•
z = torch_ops.add(x, y)
```

### 7.2 çŸ©é˜µä¹˜æ³•
```python
A = torch.randn(512, 512, device='cuda')
B = torch.randn(512, 512, device='cuda')

# è‡ªåŠ¨è°ƒä¼˜çš„çŸ©é˜µä¹˜æ³•
C = torch_ops.bmm(A.unsqueeze(0), B.unsqueeze(0)).squeeze(0)
```

### 7.3 æ³¨æ„åŠ›æœºåˆ¶
```python
query = torch.randn(2, 8, 128, 64, device='cuda')  # (batch, heads, seq, dim)
key = torch.randn(2, 8, 128, 64, device='cuda')
value = torch.randn(2, 8, 128, 64, device='cuda')

output = torch_ops.scaled_dot_product_attention(
    query, key, value,
    is_causal=True,  # è‡ªå›å½’
    scale=0.125
)
```

### 7.4 é…ç½®è°ƒä¼˜å‚æ•°
```python
from ntops.torch.utils import set_default_num_warps

# è®¾ç½®é»˜è®¤é…ç½®
set_default_num_warps(8)

# åç»­ç®—å­ä½¿ç”¨è¯¥é…ç½®
result = torch_ops.add(x, y)
```

## 8. æ‰©å±•æŒ‡å—

### 8.1 æ·»åŠ æ–°ç®—å­

**æ­¥éª¤1**: åœ¨`src/ntops/kernels/`åˆ›å»ºå†…æ ¸å®ç°
```python
# new_op.py
def application(input, output):
    output = ...  # è®¡ç®—é€»è¾‘

def arrangement(*tensors, block_size=None):
    return tuple(
        tensor.flatten().tile((block_size,))
        for tensor in tensors
    )

def premake(ndim, dtype=None, block_size=None):
    arrangement_ = functools.partial(arrangement, block_size=block_size)
    tensors = (Tensor(ndim, dtype=dtype), Tensor(ndim, dtype=dtype))
    return arrangement_, application, tensors
```

**æ­¥éª¤2**: åœ¨`src/ntops/kernels/__init__.py`å¯¼å‡º
```python
from ntops.kernels.new_op import new_op
```

**æ­¥éª¤3**: åœ¨`src/ntops/torch/`åˆ›å»ºPyTorchæ¥å£
```python
# torch/new_op.py
import torch
import ntops
from ntops.torch.utils import _cached_make

def new_op(input, *, out=None):
    if out is None:
        out = torch.empty_like(input)
    kernel = _cached_make(ntops.kernels.new_op.premake, input.ndim)
    kernel(input, out)
    return out
```

**æ­¥éª¤4**: åœ¨`src/ntops/torch/__init__.py`å¯¼å‡º
```python
from ntops.torch.new_op import new_op
```

**æ­¥éª¤5**: æ·»åŠ æµ‹è¯•
```python
# tests/test_new_op.py
def test_new_op():
    input = torch.randn(1024, device='cuda')
    output = ntops.torch.new_op(input)
    expected = torch.new_op(input)
    assert torch.allclose(output, expected)
```

### 8.2 è°ƒè¯•ç®—å­

**å¯ç”¨è¯¦ç»†æ—¥å¿—**:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

**å¯è§†åŒ–å†…å­˜å¸ƒå±€**:
```python
from ninetoothed import visualization

# åœ¨å†…æ ¸å®ç°ä¸­æ·»åŠ 
visualization.visualize(tensor)
```

**ç¬¦å·æ±‚å€¼**:
```python
from ninetoothed import eval

# æ£€æŸ¥ç¬¦å·å¼ é‡çš„å®é™…å¸ƒå±€
result = eval(tensor, {block_size: 64})
print(result)
```

## 9. é™åˆ¶ä¸æ³¨æ„äº‹é¡¹

### 9.1 ç¼–è¯‘æ—¶é™åˆ¶
- **å›ºå®šå½¢çŠ¶**: å¼ é‡ç»´åº¦å¿…é¡»åœ¨ç¼–è¯‘æ—¶ç¡®å®š
- **é¦–æ¬¡è°ƒç”¨æ…¢**: JITç¼–è¯‘æœ‰å¯åŠ¨å¼€é”€(é€šè¿‡ç¼“å­˜ç¼“è§£)

### 9.2 åŠŸèƒ½é™åˆ¶
- **ä¸æ”¯æŒåŠ¨æ€æ§åˆ¶æµ**: å¦‚`if`æ¡ä»¶ä¾èµ–äºè¿è¡Œæ—¶å€¼
- **ä¸æ”¯æŒå¤æ‚ç´¢å¼•**: å¦‚é«˜çº§ç´¢å¼•

### 9.3 æ€§èƒ½è€ƒé‡
- **å°å¼ é‡æ€§èƒ½å·®**: å†…æ ¸å¯åŠ¨å¼€é”€å¯èƒ½è¶…è¿‡è®¡ç®—æ”¶ç›Š
- **å»ºè®®**: å¯¹å°å¼ é‡ä½¿ç”¨PyTorchåŸç”Ÿæ“ä½œ

## 10. æœªæ¥æ–¹å‘

- **æ›´å¤šç®—å­**: æŒç»­æ·»åŠ å¸¸ç”¨æ·±åº¦å­¦ä¹ ç®—å­
- **æ€§èƒ½ä¼˜åŒ–**: é’ˆå¯¹æ–°GPUæ¶æ„ä¼˜åŒ–
- **æ··åˆç²¾åº¦**: æ›´å¥½çš„FP8/BF16æ”¯æŒ
- **åˆ†å¸ƒå¼**: å¤šGPUå’ŒèŠ‚ç‚¹é—´é€šä¿¡ç®—å­

---

**ç›¸å…³æ–‡æ¡£**:
- [ninetoothedç¼–è¯‘å™¨æ–‡æ¡£](../ninetoothed/README_ANALYSIS.md)
- [ntops APIå‚è€ƒ](https://github.com/InfiniTensor/ntops)

**æœ€åæ›´æ–°**: 2025-01-14
