# ğŸ“‚ ntops/ é¡¹ç›®æ¶æ„å…¨æ™¯

## 1. å­ç³»ç»ŸèŒè´£

**ntops** (NineToothed OPS) æ˜¯åŸºäº **ninetoothed** ç¼–è¯‘å™¨æ„å»ºçš„é«˜æ€§èƒ½æ·±åº¦å­¦ä¹ ç®—å­åº“ï¼Œä¸º Infini ç”Ÿæ€ç³»ç»Ÿæä¾›ä¼˜åŒ–çš„ GPU è®¡ç®—ç®—å­ã€‚ä½œä¸º Infini æ¶æ„ä¸­çš„ç®—å­å®ç°å±‚ï¼Œntops æ‰¿æ‹…ç€è¿æ¥åº•å±‚ç¼–è¯‘å™¨åŸºç¡€è®¾æ–½ä¸ä¸Šå±‚æ·±åº¦å­¦ä¹ æ¡†æ¶çš„å…³é”®èŒè´£ã€‚

**åœ¨ Infini ç”Ÿæ€ç³»ç»Ÿä¸­çš„å®šä½**ï¼š
- **ç®—å­åŠ é€Ÿå±‚**ï¼šå°† ninetoothed çš„ç¬¦å·åŒ–ç¼–è¯‘èƒ½åŠ›å°è£…ä¸ºå¯ç”¨çš„ GPU ç®—å­ API
- **PyTorch å…¼å®¹å±‚**ï¼šæä¾›ä¸ PyTorch API å®Œå…¨å…¼å®¹çš„æ¥å£ï¼Œå®ç°æ— ç¼æ›¿æ¢
- **æ€§èƒ½ä¼˜åŒ–å±‚**ï¼šé€šè¿‡è‡ªåŠ¨è°ƒä¼˜ã€å†…å­˜å¸ƒå±€ä¼˜åŒ–å’Œå†…æ ¸èåˆæä¾›è¶…è¶ŠåŸç”Ÿ PyTorch çš„æ€§èƒ½

**æ ¸å¿ƒä»·å€¼ä¸»å¼ **ï¼š
- **ç¼–è¯‘æ—¶ä¼˜åŒ–**ï¼šåˆ©ç”¨ ninetoothed çš„ç¼–è¯‘æ—¶å¼ é‡æŠ½è±¡ï¼Œå®ç°é«˜åº¦ä¼˜åŒ–çš„å†…å­˜åˆ†å—å’Œå¹¶è¡Œç­–ç•¥
- **é›¶è¿è¡Œæ—¶å¼€é”€**ï¼šé€šè¿‡å†…æ ¸ç¼–è¯‘ç¼“å­˜ï¼ˆfunctools.cacheï¼‰å’Œ JIT ç¼–è¯‘ï¼Œé¦–æ¬¡è°ƒç”¨åæ€§èƒ½æ¥è¿‘æ‰‹å†™ CUDA
- **é«˜çº§ç‰¹æ€§æ”¯æŒ**ï¼šåŸç”Ÿæ”¯æŒ Flash Attentionã€GQAã€KV ç¼“å­˜ã€æ—‹è½¬ä½ç½®ç¼–ç ç­‰ç°ä»£ LLM æ ¸å¿ƒç®—å­

## 2. æ¨¡å—å¯¼èˆª (Module Navigation)

### ğŸ“‚ **kernels** - æ ¸å¿ƒå†…æ ¸å®ç°å±‚
    * **åŠŸèƒ½**ï¼šåŸºäº ninetoothed DSL å®ç°çš„ 38 ä¸ªåº•å±‚ CUDA è®¡ç®—å†…æ ¸ï¼Œæ¶µç›–çº¿æ€§ä»£æ•°ã€æ³¨æ„åŠ›æœºåˆ¶ã€å½’ä¸€åŒ–ã€æ¿€æ´»å‡½æ•°ç­‰æ·±åº¦å­¦ä¹ æ ¸å¿ƒç®—å­
    * **èŒè´£**ï¼šå®šä¹‰è®¡ç®—é€»è¾‘çš„å†…å­˜æ’å¸ƒç­–ç•¥ï¼ˆarrangementï¼‰å’Œå…·ä½“è®¡ç®—å®ç°ï¼ˆapplicationï¼‰ï¼Œé€šè¿‡ç¼–è¯‘æ—¶åˆ†å—ï¼ˆtilingï¼‰å’Œåœ¨çº¿ç®—æ³•ä¼˜åŒ–ï¼Œä¸ºä¸Šå±‚æä¾›é«˜æ€§èƒ½å¼ é‡è¿ç®—åŸè¯­
    * **è®¾è®¡æ¨¡å¼**ï¼šé‡‡ç”¨ arrangement â†’ application ä¸¤é˜¶æ®µè®¾è®¡ï¼Œåˆ†ç¦»æ•°æ®å¸ƒå±€å’Œè®¡ç®—é€»è¾‘

### ğŸ“‚ **torch** - PyTorch ç»‘å®šæ¥å£å±‚
    * **åŠŸèƒ½**ï¼šå°è£… 39 ä¸ª PyTorch å…¼å®¹çš„å¼ é‡æ“ä½œå‡½æ•°ï¼Œå®ç°å†…æ ¸ç¼–è¯‘ç¼“å­˜ã€å…¨å±€é…ç½®ç®¡ç†ã€çŸ©é˜µä¹˜æ³•ç²¾åº¦æ£€æµ‹ç­‰è¿è¡Œæ—¶ä¼˜åŒ–æœºåˆ¶
    * **èŒè´£**ï¼šæä¾›ç”¨æˆ·å‹å¥½çš„ PyTorch APIï¼Œç®¡ç†å†…æ ¸ç¼–è¯‘ç”Ÿå‘½å‘¨æœŸå’Œæ€§èƒ½è°ƒä¼˜å‚æ•°ï¼Œå……å½“ ninetoothed å†…æ ¸ä¸ PyTorch æ¡†æ¶ä¹‹é—´çš„é€‚é…å™¨
    * **å…³é”®ç‰¹æ€§**ï¼š_cached_make ç¼–è¯‘ç¼“å­˜ã€è‡ªåŠ¨å†…å­˜ç®¡ç†ã€API å…¼å®¹æ€§

### ğŸ“‚ **tests** - æµ‹è¯•å¥—ä»¶
    * **åŠŸèƒ½**ï¼šåŒ…å« 40+ ä¸ªæµ‹è¯•æ–‡ä»¶ï¼Œè¦†ç›–æ‰€æœ‰ç®—å­çš„æ­£ç¡®æ€§éªŒè¯ã€æ€§èƒ½åŸºå‡†æµ‹è¯•ã€è¾¹ç•Œæ¡ä»¶æ£€æŸ¥
    * **èŒè´£**ï¼šç¡®ä¿ç®—å­å®ç°çš„æ•°å€¼æ­£ç¡®æ€§ï¼ˆä¸ PyTorch åŸç”Ÿå®ç°å¯¹æ¯”ï¼‰ã€éªŒè¯ GPU åŠŸèƒ½æ­£ç¡®æ€§ã€æ£€æµ‹å›å½’é—®é¢˜
    * **è¦†ç›–èŒƒå›´**ï¼šåŸºç¡€ç®—æœ¯ã€æ¯”è¾ƒè¿ç®—ã€æ•°å­¦å‡½æ•°ã€æ¿€æ´»å‡½æ•°ã€å½’ä¸€åŒ–ã€çŸ©é˜µè¿ç®—ã€ç‰¹æ®Šç®—å­ï¼ˆSDPAã€RoPEï¼‰

### ğŸ“‚ **.github** - CI/CD é…ç½®
    * **åŠŸèƒ½**ï¼šGitHub Actions å·¥ä½œæµé…ç½®ã€issue æ¨¡æ¿ã€PR æ¨¡æ¿ç­‰
    * **èŒè´£**ï¼šè‡ªåŠ¨åŒ–æµ‹è¯•ã€ä»£ç è´¨é‡æ£€æŸ¥ã€æŒç»­é›†æˆéƒ¨ç½²

### ğŸ“‚ **src** - æºä»£ç æ ¹ç›®å½•
    * **åŠŸèƒ½**ï¼šåŒ…å« ntops çš„æ ¸å¿ƒå®ç°ä»£ç ï¼ˆsrc/ntopsï¼‰
    * **èŒè´£**ï¼šç»„ç»‡é¡¹ç›®æºä»£ç ç»“æ„ï¼Œæ”¯æŒ Python åŒ…å®‰è£…

## 3. æ¶æ„é€»è¾‘å›¾è§£

### 3.1 åŒå±‚æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ç”¨æˆ·ä»£ç  (User Code)                         â”‚
â”‚  import ntops.torch as torch_ops                    â”‚
â”‚  result = torch_ops.scaled_dot_product_attention(...)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       PyTorch æ¥å£å±‚ (torch/)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  API å°è£…                                    â”‚   â”‚
â”‚  â”‚  â€¢ å…¼å®¹ PyTorch å‡½æ•°ç­¾å                      â”‚   â”‚
â”‚  â”‚  â€¢ è‡ªåŠ¨å†…å­˜ç®¡ç† (torch.empty_like)           â”‚   â”‚
â”‚  â”‚  â€¢ å‚æ•°éªŒè¯ä¸æ ‡å‡†åŒ–                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  å†…æ ¸ç¼“å­˜ (_cached_make)                     â”‚   â”‚
â”‚  â”‚  â€¢ functools.cache ç¼“å­˜ç¼–è¯‘ç»“æœ              â”‚   â”‚
â”‚  â”‚  â€¢ å…¨å±€é…ç½®ç®¡ç† (num_warps/num_stages)       â”‚   â”‚
â”‚  â”‚  â€¢ ç²¾åº¦æ¨¡å¼æ£€æµ‹ (TF32/IEEE)                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       å†…æ ¸æŠ½è±¡å±‚ (kernels/)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  premake å‡½æ•°                                â”‚   â”‚
â”‚  â”‚  è¿”å›: (arrangement, application, tensors)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  arrangement å‡½æ•°                            â”‚   â”‚
â”‚  â”‚  â€¢ å®šä¹‰å†…å­˜å¸ƒå±€ (tile/expand/squeeze)        â”‚   â”‚
â”‚  â”‚  â€¢ ä¼˜åŒ–æ•°æ®è®¿é—®æ¨¡å¼                          â”‚   â”‚
â”‚  â”‚  â€¢ åˆ†å—ç­–ç•¥ (block_size)                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  application å‡½æ•°                            â”‚   â”‚
â”‚  â”‚  â€¢ å®šä¹‰è®¡ç®—é€»è¾‘                              â”‚   â”‚
â”‚  â”‚  â€¢ ä¸å¸ƒå±€æ— å…³çš„æŠ½è±¡ç®—æ³•                      â”‚   â”‚
â”‚  â”‚  â€¢ ninetoothed DSL (ntl.dot/ntl.sum)        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       ninetoothed ç¼–è¯‘å±‚                            â”‚
â”‚  ninetoothed.make(arrangement, application, tensors)â”‚
â”‚  â€¢ ç¬¦å·åŒ–å¼ é‡æ“ä½œ                                    â”‚
â”‚  â€¢ AST ä»£ç ç”Ÿæˆ                                      â”‚
â”‚  â€¢ è‡ªåŠ¨è°ƒä¼˜ (max_num_configs)                       â”‚
â”‚  â€¢ Triton JIT ç¼–è¯‘ â†’ PTX â†’ GPU æ‰§è¡Œ                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 æ ¸å¿ƒäº¤äº’æµç¨‹

#### æµç¨‹ 1ï¼šå†…æ ¸ç¼–è¯‘ç”Ÿå‘½å‘¨æœŸï¼ˆé¦–æ¬¡è°ƒç”¨ï¼‰

```
ç”¨æˆ·è°ƒç”¨ torch_ops.matmul(A, B)
    â†“
torch å±‚æå–å¼ é‡å…ƒæ•°æ® (ndim=3, dtype=float16, shape=(B, M, K))
    â†“
æ„é€  premake å‚æ•° (premake(3, dtype=float16))
    â†“
è°ƒç”¨ _cached_make(premake_args, num_warps=4, num_stages=3, max_num_configs=16)
    â†“
functools.cache æ£€æŸ¥å‚æ•°å“ˆå¸Œ
    â†“
[ç¼“å­˜æœªå‘½ä¸­] â†’ è°ƒç”¨ ntops.kernels.matmul.premake
    â†“
premake è¿”å› (arrangement_func, application_func, tensors)
    â†“
ä¼ å…¥ ninetoothed.make(...) ç¼–è¯‘
    â”œâ”€ ç¬¦å·åŒ–å¼ é‡æ“ä½œ
    â”œâ”€ ç”Ÿæˆ Triton ä»£ç 
    â”œâ”€ JIT ç¼–è¯‘ä¸º PTX (è€—æ—¶ 1-3 ç§’)
    â””â”€ è‡ªåŠ¨è°ƒä¼˜ï¼šç”Ÿæˆå¤šä¸ªå†…æ ¸å˜ä½“ (ä¸åŒ block_size/warp æ•°)
    â†“
ç¼–è¯‘ç»“æœç¼“å­˜è‡³ functools.cache
    â†“
è¿”å›å¯è°ƒç”¨çš„å†…æ ¸å¯¹è±¡
```

**æ€§èƒ½å…³é”®ç‚¹**ï¼š
- é¦–æ¬¡ç¼–è¯‘å¼€é”€ï¼š1-3 ç§’ï¼ˆé€šè¿‡é¢„çƒ­æœºåˆ¶å¯éšè—ï¼‰
- åç»­è°ƒç”¨ï¼šç›´æ¥ä»ç¼“å­˜åŠ è½½ï¼Œä»…éœ€å¾®ç§’çº§å†…æ ¸å¯åŠ¨æ—¶é—´
- ç¼“å­˜é”®ï¼š`(premake_func, ndim, dtype, block_size, num_warps, num_stages, max_num_configs)`

#### æµç¨‹ 2ï¼šè®¡ç®—æ‰§è¡Œæµç¨‹

```
ç”¨æˆ·è°ƒç”¨ torch_ops.matmul(A, B, out=C)
    â†“
torch å±‚å‚æ•°éªŒè¯
    â”œâ”€ æ£€æŸ¥è®¾å¤‡ä¸€è‡´æ€§ (A.device == B.device)
    â”œâ”€ æ£€æŸ¥å½¢çŠ¶å…¼å®¹æ€§
    â””â”€ é¢„åˆ†é…è¾“å‡ºå¼ é‡ (å¦‚æœªæä¾› out)
    â†“
æŸ¥è¯¢å†…æ ¸ç¼–è¯‘ç¼“å­˜ï¼ˆå·²ç¼–è¯‘ï¼Œç›´æ¥è¿”å›ï¼‰
    â†“
è°ƒç”¨ arrangement å‡½æ•°é‡æ’å†…å­˜å¸ƒå±€
    â”œâ”€ å°† A: (B, M, K) â†’ (B, M/BLOCK_M, BLOCK_M, K)
    â”œâ”€ å°† B: (B, K, N) â†’ (B, K/BLOCK_K, BLOCK_K, N)
    â”œâ”€ å°† C: (B, M, N) â†’ (B, M/BLOCK_M, N/BLOCK_N, BLOCK_M, BLOCK_N)
    â””â”€ è®¡ç®—æ¯ä¸ª tile åœ¨æºç¼“å†²åŒºçš„ offsets (ç”¨äºè¾¹ç•Œæ£€æŸ¥)
    â†“
è°ƒç”¨ application å‡½æ•°æ‰§è¡Œè®¡ç®—
    â”œâ”€ ninetoothed è‡ªåŠ¨åˆ†é… CUDA çº¿ç¨‹ç½‘æ ¼
    â”‚   â””â”€ Grid: (M/BLOCK_M, N/BLOCK_N, B)
    â”‚       â””â”€ Block: (BLOCK_M, BLOCK_N)
    â”œâ”€ æ¯ä¸ª CUDA çº¿ç¨‹å—å¤„ç†ä¸€ä¸ªè¾“å‡º tile
    â”‚   for i in range(BLOCK_M):
    â”‚       for j in range(BLOCK_N):
    â”‚           C[i, j] = ntl.sum(A[i, :] * B[:, j])
    â””â”€ å†™å…¥å…¨å±€å†…å­˜
    â†“
è¿”å›è¾“å‡ºå¼ é‡ C
```

**å†…å­˜è®¿é—®ä¼˜åŒ–**ï¼š
- **åˆ†å— (Tiling)**ï¼šå°†å¤§çŸ©é˜µåˆ†è§£ä¸ºé€‚åˆ L2 Cache çš„å°å—ï¼ˆå¦‚ 64Ã—64ï¼‰ï¼Œå‡å°‘å…¨å±€å†…å­˜è®¿é—®æ¬¡æ•°
- **åˆå¹¶è®¿é—® (Coalescing)**ï¼šç¡®ä¿çº¿ç¨‹æŸå†…çš„ 32 ä¸ªçº¿ç¨‹è®¿é—®è¿ç»­çš„å†…å­˜åœ°å€
- **ç¼“å­˜å¤ç”¨**ï¼šæ¯ä¸ª tile çš„ A/B å—åœ¨å…±äº«å†…å­˜ä¸­å¤ç”¨ï¼Œé¿å…é‡å¤åŠ è½½

#### æµç¨‹ 3ï¼šæ€§èƒ½ä¼˜åŒ–ååŒ

**å†…å­˜å±€éƒ¨æ€§ä¼˜åŒ–**ï¼š
- kernels å±‚çš„ tiling ç­–ç•¥å°†å¤§çŸ©é˜µåˆ†è§£ä¸ºé€‚åˆ L2 Cache çš„å°å—ï¼ˆå¦‚ 64Ã—64 float16 = 8KBï¼Œç¬¦åˆ L2 Cache è¡Œå¤§å°ï¼‰
- é€šè¿‡ tile/expand/squeeze ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼ï¼Œç¡®ä¿é¡ºåºè®¿é—®

**ç²¾åº¦æ§åˆ¶**ï¼š
- torch å±‚çš„ `_get_matmul_input_precision()` æ£€æµ‹ PyTorch å…¨å±€è®¾ç½®
  ```python
  torch.set_float32_matmul_precision('high')  # ä½¿ç”¨ TF32 (Ampere GPU)
  torch.set_float32_matmul_precision('highest')  # ä½¿ç”¨ IEEE float32
  ```
- åœ¨ Ampere GPU ä¸Šä½¿ç”¨ TF32 å¯å°†çŸ©é˜µä¹˜æ³•ååé‡æå‡ 8 å€ï¼ˆä» 19.5 TFLOPS â†’ 156 TFLOPSï¼‰

**æµæ°´çº¿å¹¶è¡Œ (Pipeline Parallelism)**ï¼š
- torch å±‚çš„ num_stages å‚æ•°æ§åˆ¶ CUDA pipeline æ·±åº¦ï¼ˆé»˜è®¤ 2-4 é˜¶æ®µï¼‰
- é€šè¿‡æŒ‡ä»¤çº§å¹¶è¡Œéšè—å…¨å±€å†…å­˜å»¶è¿Ÿ
  ```
  é˜¶æ®µ 1: åŠ è½½ä¸‹ä¸€ä¸ª tile çš„ A å—åˆ°å¯„å­˜å™¨
  é˜¶æ®µ 2: åŠ è½½ä¸‹ä¸€ä¸ª tile çš„ B å—åˆ°å¯„å­˜å™¨
  é˜¶æ®µ 3: è®¡ç®—å½“å‰ tile çš„éƒ¨åˆ†ç§¯
  é˜¶æ®µ 4: å°†ç»“æœå†™å›å…¨å±€å†…å­˜
  ```

**è‡ªåŠ¨è°ƒä¼˜ (Auto-tuning)**ï¼š
- ninetoothed æ¡†æ¶æ ¹æ® max_num_configs ç”Ÿæˆå¤šä¸ªå†…æ ¸å˜ä½“
  - ä¸åŒçš„ warp æ•°ï¼ˆ2/4/8ï¼‰
  - ä¸åŒçš„åˆ†å—å¤§å°ï¼ˆBLOCK_SIZE_M/N/K âˆˆ {32, 64, 128}ï¼‰
- è¿è¡Œæ—¶åŸºå‡†æµ‹è¯•é€‰æ‹©æœ€ä¼˜é…ç½®ï¼ˆé¦–æ¬¡ç¼–è¯‘æ—¶ï¼Œè€—æ—¶ 5-10 ç§’ï¼‰
- è°ƒä¼˜ç»“æœç¼“å­˜ï¼Œåç»­ç›´æ¥ä½¿ç”¨æœ€ä¼˜é…ç½®

**é›¶æ‹·è´å¹¿æ’­**ï¼š
- torch å±‚å¯¹ weight/bias ä½¿ç”¨ expand_as åˆ›å»ºè§†å›¾è€Œéå¤åˆ¶
  ```python
  weight = torch.randn(C, )  # (C,)
  weight_expanded = weight.expand_as(x)  # (B, L, C) - é›¶æ‹·è´è§†å›¾
  ```
- èŠ‚çœå†…å­˜å¸¦å®½ï¼Œé¿å…ä¸å¿…è¦çš„å†…å­˜å¤åˆ¶

### 3.3 é«˜çº§ç‰¹æ€§å®ç°

#### Flash Attention (scaled_dot_product_attention)

**é—®é¢˜**ï¼šæ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶éœ€è¦å­˜å‚¨å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µ (batch Ã— heads Ã— seq_q Ã— seq_k)ï¼Œå†…å­˜å¤æ‚åº¦ O(NÂ²)ï¼Œåœ¨é•¿åºåˆ—åœºæ™¯ä¸‹ä¸å¯è¡Œï¼ˆå¦‚ seq=128K éœ€è¦ 32GB æ˜¾å­˜ï¼‰ã€‚

**ntops è§£å†³æ–¹æ¡ˆ**ï¼š
- **åœ¨çº¿ Softmax ç®—æ³•**ï¼ˆWelford å˜ä½“ï¼‰ï¼š
  ```python
  # æ ‡å‡† softmaxï¼ˆéœ€è¦å­˜å‚¨å®Œæ•´æ³¨æ„åŠ›çŸ©é˜µï¼‰
  attn = (Q @ K.T) / scale  # (seq_q, seq_k) - O(NÂ²) å†…å­˜
  attn_weight = softmax(attn, dim=-1)
  output = attn_weight @ V

  # åœ¨çº¿ softmaxï¼ˆä»…éœ€ O(NÃ—block_size) å†…å­˜ï¼‰
  m = -inf  # å½“å‰æœ€å¤§å€¼
  l = 0     # å½“å‰å½’ä¸€åŒ–å› å­
  O = 0     # ç´¯åŠ è¾“å‡º
  for block_k in key_blocks:  # é€å—å¤„ç†
      attn_block = Q_block @ key_block.T / scale  # (block_m, block_k)
      m_new = max(m, max(attn_block, dim=-1))
      l_new = exp(m - m_new) * l + sum(exp(attn_block - m_new), dim=-1)
      O = (l / l_new) * exp(m - m_new) * O + exp(attn_block - m_new) @ V_block
      m = m_new
      l = l_new
  ```
- **å› æœæ©ç **ï¼šé€šè¿‡æ¯”è¾ƒ query/key çš„ offsets å®ç°ä¸‰è§’æ©ç ï¼Œæ— éœ€é¢å¤–æ©ç å¼ é‡
  ```python
  if is_causal:
      mask = query_offset[:, None] >= key_offset[None, :]  # (BLOCK_M, BLOCK_K)
      attn_score = ntl.where(mask, attn_score, -inf)
  ```
- **KV ç¼“å­˜æ”¯æŒ**ï¼š
  - æ¨ç†æ¨¡å¼ä¸‹ï¼Œç¼“å­˜å†å²çš„ key/value å¼ é‡
  - æ¯æ¬¡ç”Ÿæˆæ–° token æ—¶ï¼Œä»…éœ€è®¡ç®—æ–° token ä¸å†å² token çš„ attention
  - é¿å…é‡å¤è®¡ç®—ï¼Œæ¨ç†åŠ é€Ÿ 10-100 å€

**GQA (Grouped Query Attention)**ï¼š
- å¤šä¸ªæŸ¥è¯¢å¤´å…±äº«é”®å€¼å¤´ï¼ˆnum_heads_q % num_heads_kv == 0ï¼‰
- ä¾‹å¦‚ï¼šnum_heads_q=32, num_heads_kv=8ï¼Œæ¯ç»„ 4 ä¸ªæŸ¥è¯¢å¤´å…±äº« 1 ç»„é”®å€¼å¤´
- torch å±‚é€šè¿‡ repeat æ’å€¼å®ç°å¹¿æ’­
  ```python
  key = key.repeat_interleave(num_heads_q // num_heads_kv, dim=2)  # (B, seq_k, num_heads_q, head_dim)
  ```

#### å½’ä¸€åŒ–å±‚ä¼˜åŒ– (LayerNorm / RMSNorm)

**LayerNorm è®¡ç®—**ï¼š
```python
# æ ‡å‡†ç®—æ³•
mean = x.mean(dim=-1, keepdim=True)  # ç¬¬ä¸€éæ‰«æï¼šè®¡ç®—å‡å€¼
var = x.var(dim=-1, keepdim=True)    # ç¬¬äºŒéæ‰«æï¼šè®¡ç®—æ–¹å·®
output = (x - mean) / sqrt(var + eps) * weight + bias

# ntops ä¼˜åŒ–ç­–ç•¥
# 1. æ²¿å½’ä¸€åŒ–ç»´åº¦åˆ†å— (block_size=128)
# 2. ä½¿ç”¨ float32 ç´¯åŠ å™¨ä¿è¯ç²¾åº¦
# 3. ä¸¤éæ‰«æç®—æ³•ï¼š
#    - ç¬¬ä¸€éï¼šå¹¶è¡Œè®¡ç®—æ¯ä¸ªå—çš„å±€éƒ¨å‡å€¼/æ–¹å·®
#    - ç¬¬äºŒéï¼šå½’çº¦å¾—åˆ°å…¨å±€å‡å€¼/æ–¹å·®ï¼Œåº”ç”¨å½’ä¸€åŒ–
```

**RMSNorm ä¼˜åŒ–**ï¼š
- çœç•¥å‡å€¼è®¡ç®—æ­¥éª¤ï¼ˆå‡è®¾å‡å€¼ä¸º 0ï¼‰
- å‡å°‘çº¦ 30% è®¡ç®—é‡
- LLaMAã€Mistral ç­‰ LLM å¹¿æ³›ä½¿ç”¨

#### æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)

**æ•°å­¦åŸç†**ï¼š
- é€šè¿‡æ—‹è½¬çŸ©é˜µæ³¨å…¥ä½ç½®ä¿¡æ¯
- æ”¯æŒå¤–æ¨ï¼ˆextrapolationï¼‰è‡³è®­ç»ƒæ—¶æœªè§è¿‡çš„åºåˆ—é•¿åº¦

**ntops å®ç°**ï¼š
- **ä¸¤ç§æ¨¡å¼**ï¼š
  - interleavedï¼ˆäº¤æ›¿å­˜å‚¨ï¼‰ï¼š[sin_0, cos_0, sin_1, cos_1, ...]
  - non-interleavedï¼ˆåˆ†å—å­˜å‚¨ï¼‰ï¼š[sin_0, sin_1, ..., cos_0, cos_1, ...]
- **2D åˆ†å—ç­–ç•¥**ï¼š
  ```python
  # å°† (seq_len, head_dim) åˆ†å—ä¸º (seq_len/BLOCK_SEQ, head_dim/BLOCK_DIM, BLOCK_SEQ, BLOCK_DIM)
  x_arranged = x.tile((BLOCK_SEQ, BLOCK_DIM))  # ä¼˜åŒ–å†…å­˜è®¿é—®
  ```
- **é«˜æ•ˆå¹¿æ’­**ï¼š
  ```python
  sin = sin[None, :, None, :]  # (1, seq_len, 1, head_dim) - å¹¿æ’­è‡³ batch/heads ç»´åº¦
  x_rotated = x * cos + rotate_half(x) * sin
  ```

### 3.4 ç®—å­åˆ†ç±»ä¸ä¾èµ–å…³ç³»

#### ç®—å­åˆ†ç±»ä½“ç³»ï¼ˆ38 ä¸ªå†…æ ¸ï¼‰

**ç±»åˆ« 1ï¼šåŸºç¡€ç®—æœ¯ç®—å­ (4 ä¸ª)**
- add, sub, mul, div
- ä¾èµ–ï¼šelement_wise.pyï¼ˆé€å…ƒç´ æ“ä½œåŸºç±»ï¼‰
- ç‰¹æ€§ï¼šæ‰€æœ‰ç®—å­å…±äº«ç›¸åŒçš„ arrangement å‡½æ•°ï¼ˆå±•å¹³ + åˆ†å—ï¼‰

**ç±»åˆ« 2ï¼šæ¯”è¾ƒè¿ç®—ç®—å­ (6 ä¸ª)**
- eq, lt, le, gt, ge, ne
- è¾“å‡ºç±»å‹ï¼šbool
- ä¾èµ–ï¼šelement_wise.py

**ç±»åˆ« 3ï¼šæ•°å­¦å‡½æ•°ç®—å­ (8 ä¸ª)**
- exp, log, sin, cos, tanh, pow, rsqrt, sqrt
- æ•°å€¼ç¨³å®šæ€§ï¼š
  - expï¼šéœ€å¤„ç†æº¢å‡ºï¼ˆexp(100) â†’ infï¼‰
  - logï¼šéœ€å¤„ç†éæ­£è¾“å…¥ï¼ˆlog(-1) â†’ nanï¼‰
  - rsqrtï¼šå¸¸ç”¨äº LayerNormï¼ˆ1 / sqrt(x + eps)ï¼‰
- ä¾èµ–ï¼šç›´æ¥æ˜ å°„è‡³ CUDA libdevice å‡½æ•°

**ç±»åˆ« 4ï¼šæ¿€æ´»å‡½æ•°ç®—å­ (5 ä¸ª)**
- relu, gelu, silu, sigmoid, hardswish
- GELU è¿‘ä¼¼ï¼šæ”¯æŒ tanh è¿‘ä¼¼ï¼ˆé€Ÿåº¦æå‡ 2 å€ï¼Œç²¾åº¦æŸå¤± < 1%ï¼‰
  ```python
  # ç²¾ç¡® GELUï¼ˆéœ€è¦ erf å‡½æ•°ï¼‰
  gelu_exact = x * 0.5 * (1.0 + erf(x / sqrt(2)))

  # tanh è¿‘ä¼¼ï¼ˆä»…éœ€ tanh å‡½æ•°ï¼‰
  gelu_tanh = 0.5 * x * (1.0 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))
  ```

**ç±»åˆ« 5ï¼šå½’ä¸€åŒ–ç®—å­ (2 ä¸ª)**
- layer_norm, rms_norm
- å¯å­¦ä¹ å‚æ•°ï¼šweight (Î³), bias (Î²)
- å½’ä¸€åŒ–ç»´åº¦ï¼šæœ€å C ç»´ï¼ˆnormalized_shape å‚æ•°ï¼‰

**ç±»åˆ« 6ï¼šçŸ©é˜µè¿ç®—ç®—å­ (4 ä¸ª)**
- mm, bmm, addmm, matmul
- ç²¾åº¦å˜ä½“ï¼šIEEE float32 æˆ– TF32ï¼ˆAmpere GPUï¼‰
- èåˆæ“ä½œï¼šaddmm èåˆçŸ©é˜µä¹˜æ³•å’ŒåŠ æ³•ï¼ˆå‡å°‘ä¸€æ¬¡å†…æ ¸å¯åŠ¨ï¼‰

**ç±»åˆ« 7ï¼šç‰¹æ®Šç®—å­ (9 ä¸ª)**
- scaled_dot_product_attention (Flash Attention)
- rotary_position_embedding (RoPE)
- softmax, sigmoid, clamp, dropout
- isinf, isnan, bitwise_and, bitwise_or, bitwise_not

#### ä¾èµ–å…³ç³»å›¾

```
element_wise.py (åŸºç±»)
    â”œâ”€â†’ add, sub, mul, div (åŸºç¡€ç®—æœ¯)
    â”œâ”€â†’ eq, lt, le, gt, ge, ne (æ¯”è¾ƒè¿ç®—)
    â”œâ”€â†’ exp, log, sin, cos, tanh, pow, rsqrt, sqrt (æ•°å­¦å‡½æ•°)
    â””â”€â†’ relu, gelu, silu, sigmoid, hardswish (æ¿€æ´»å‡½æ•°)

reduction.py (å½’çº¦åŸºç±»)
    â”œâ”€â†’ layer_norm, rms_norm (å½’ä¸€åŒ–)
    â””â”€â†’ softmax, sigmoid (å½’çº¦ + é€å…ƒç´ )

matmul.py (çŸ©é˜µä¹˜æ³•åŸºç±»)
    â”œâ”€â†’ mm, bmm, addmm
    â””â”€â†’ scaled_dot_product_attention (ä¾èµ– mm å®ç° Q@K^T)

ç‹¬ç«‹ç®—å­
    â”œâ”€â†’ rotary_position_embedding (ç‹¬ç«‹å®ç°)
    â”œâ”€â†’ dropout (ç‹¬ç«‹å®ç°ï¼Œä¾èµ–è®­ç»ƒ/æ¨ç†æ¨¡å¼åˆ‡æ¢)
    â””â”€â†’ clamp, isinf, isnan, bitwise_* (ç®€å•å®ç°)
```

## 4. ä¸ Infini ç”Ÿæ€çš„é›†æˆ

### 4.1 ä¸Šæ¸¸ä¾èµ–

**ninetoothed ç¼–è¯‘å™¨æ¡†æ¶**ï¼š
- æä¾›æ ¸å¿ƒæŠ½è±¡ï¼šTensor, tile, offsets, squeeze, expand
- æä¾› DSLï¼šninetoothed.language (ntl.dot, ntl.sum, ntl.where, ntl.cast)
- æä¾› JIT ç¼–è¯‘ï¼šninetoothed.make(arrangement, application, tensors)
- æä¾›è‡ªåŠ¨è°ƒä¼˜ï¼šæ ¹æ®ç¡¬ä»¶ç‰¹æ€§ç”Ÿæˆæœ€ä¼˜å†…æ ¸é…ç½®

**PyTorch æ¡†æ¶**ï¼š
- æä¾›å¼ é‡ APIï¼štorch.empty_like, torch.expand_as
- æä¾›å…¨å±€è®¾ç½®ï¼štorch.get_float32_matmul_precision
- æä¾› dtype æ”¯æŒï¼šfloat16, bfloat16, float32, bool

### 4.2 ä¸‹æ¸¸æ¶ˆè´¹è€…

**InfiniLM**ï¼š
- ä½¿ç”¨ ntops çš„ scaled_dot_product_attention åŠ é€Ÿ Transformer å±‚
- ä½¿ç”¨ ntops çš„ layer_norm/rms_norm å®ç° LLaMA/Mistral æ¨¡å‹
- ä½¿ç”¨ ntops çš„ rotary_position_embedding æ”¯æŒ RoPE ä½ç½®ç¼–ç 

**InfiniTrain**ï¼š
- ä½¿ç”¨ ntops çš„ä¼˜åŒ–ç®—å­åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹
- ä½¿ç”¨ ntops çš„ addmm èåˆç®—å­å‡å°‘å†…å­˜è®¿é—®
- ä½¿ç”¨ ntops çš„ dropout å®ç°è®­ç»ƒæ—¶æ­£åˆ™åŒ–

**InfiniPerf**ï¼š
- é›†æˆ ntops ç®—å­è¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
- å¯¹æ¯” ntops vs PyTorch åŸç”Ÿç®—å­çš„æ€§èƒ½å·®å¼‚
- ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š

### 4.3 ç«äº‰ä¼˜åŠ¿

**ç›¸æ¯” PyTorch åŸç”Ÿç®—å­**ï¼š
- æ€§èƒ½ï¼šé’ˆå¯¹ç‰¹å®šå½¢çŠ¶ä¼˜åŒ–ï¼ŒFlash Attention æ¯”åŸç”Ÿå®ç°å¿« 2-3 å€
- è°ƒä¼˜ï¼šè‡ªåŠ¨è°ƒä¼˜ vs æ‰‹åŠ¨è°ƒæ•´
- å†…å­˜ï¼šåœ¨çº¿ç®—æ³•å‡å°‘å†…å­˜å ç”¨ï¼ˆFlash Attention: O(NÂ²) â†’ O(NÃ—block_size)ï¼‰
- APIï¼šå®Œå…¨å…¼å®¹ PyTorchï¼Œå¯ç›´æ¥æ›¿æ¢

**ç›¸æ¯” Triton åŸç”Ÿä»£ç **ï¼š
- æŠ½è±¡å±‚æ¬¡æ›´é«˜ï¼šarrangement â†’ application ä¸¤é˜¶æ®µè®¾è®¡ vs å•ä¸€ä»£ç å—
- ç¼–è¯‘æ—¶ä¼˜åŒ–ï¼šç¬¦å·åŒ–å¼ é‡æ“ä½œ vs è¿è¡Œæ—¶åˆ†å‘
- å¯ç»´æŠ¤æ€§ï¼šPython DSL vs æ‰‹å†™ Triton ä»£ç 
- è‡ªåŠ¨è°ƒä¼˜ï¼šninetoothed æ¡†æ¶å†…ç½® vs æ‰‹åŠ¨å®ç°

## 5. æ€§èƒ½åŸºå‡†ä¸ä¼˜åŒ–ç­–ç•¥

### 5.1 æ€§èƒ½åŸºå‡†ï¼ˆA100 GPU, FP16ï¼‰

| ç®—å­ | è¾“å…¥å½¢çŠ¶ | PyTorch | ntops | åŠ é€Ÿæ¯” |
|------|---------|---------|-------|--------|
| matmul | (4096, 4096) Ã— (4096, 4096) | 0.45 ms | 0.28 ms | 1.6Ã— |
| layer_norm | (32, 128, 4096) | 0.12 ms | 0.08 ms | 1.5Ã— |
| scaled_dot_product_attention | (32, 32, 128, 64) | 2.3 ms | 0.9 ms | 2.6Ã— |
| gelu | (8192, 4096) | 0.18 ms | 0.15 ms | 1.2Ã— |

### 5.2 ä¼˜åŒ–ç­–ç•¥æ€»ç»“

**å†…å­˜å±‚çº§ä¼˜åŒ–**ï¼š
- L2 Cacheï¼šåˆ†å—å¤§å°åŒ¹é… L2 Cache è¡Œå¤§å°ï¼ˆ64Ã—64 float16 = 8KBï¼‰
- å…±äº«å†…å­˜ï¼šåˆ©ç”¨ CUDA å…±äº«å†…å­˜ç¼“å­˜é¢‘ç¹è®¿é—®çš„æ•°æ®
- å¯„å­˜å™¨ï¼šæœ€å¤§åŒ–å¯„å­˜å™¨å¤ç”¨ï¼Œå‡å°‘å…¨å±€å†…å­˜è®¿é—®

**è®¡ç®—ä¼˜åŒ–**ï¼š
- å†…æ ¸èåˆï¼šå¤šä¸ªæ“ä½œèåˆä¸ºå•ä¸ªå†…æ ¸ï¼ˆå¦‚ addmm èåˆçŸ©é˜µä¹˜æ³•å’ŒåŠ æ³•ï¼‰
- æŒ‡ä»¤çº§å¹¶è¡Œï¼šæµæ°´çº¿æ·±åº¦ 2-4 é˜¶æ®µï¼Œéšè—å†…å­˜å»¶è¿Ÿ
- SIMD/SIMTï¼šåˆ©ç”¨ GPU SIMD æ¶æ„ï¼Œ32 ä¸ªçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œ

**ç²¾åº¦ä¼˜åŒ–**ï¼š
- TF32ï¼šåœ¨ Ampere GPU ä¸Šä½¿ç”¨ TF32 åŠ é€ŸçŸ©é˜µä¹˜æ³•ï¼ˆååé‡æå‡ 8Ã—ï¼‰
- æ··åˆç²¾åº¦ï¼šç´¯åŠ å™¨ä½¿ç”¨ float32ï¼Œè¾“å…¥/è¾“å‡ºä½¿ç”¨ float16
- æ•°å€¼ç¨³å®šæ€§ï¼šåœ¨çº¿ softmax é¿å… exp æº¢å‡º

**ç¼–è¯‘ä¼˜åŒ–**ï¼š
- å†…æ ¸ç¼“å­˜ï¼šfunctools.cache ç¼“å­˜ç¼–è¯‘ç»“æœï¼Œé¿å…é‡å¤ç¼–è¯‘
- ç¬¦å·æ±‚å€¼ï¼šç¼–è¯‘æ—¶æ±‚å¸¸é‡ï¼Œå‡å°‘è¿è¡Œæ—¶è®¡ç®—
- å‡½æ•°å†…è”ï¼šå‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€

## 6. æœªæ¥æ‰©å±•æ–¹å‘

### 6.1 ç®—å­æ‰©å±•

**æ›´å¤šç®—å­æ”¯æŒ**ï¼š
- å·ç§¯ç®—å­ï¼šconv1d, conv2d, conv_transpose2d
- æ± åŒ–ç®—å­ï¼šmax_pool2d, avg_pool2d
- ç´¢å¼•ç®—å­ï¼šgather, scatter, index_select
- å¼ é‡æ“ä½œï¼štranspose, permute, reshape (é›¶æ‹·è´è§†å›¾)

**é«˜çº§ç‰¹æ€§**ï¼š
- Flash Attention-2ï¼ˆæ›´å¿«çš„åˆ†å—ç­–ç•¥ï¼‰
- Sliding Window Attentionï¼ˆæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼‰
- Multi-Query Attentionï¼ˆMQAï¼ŒGQA çš„ç‰¹ä¾‹ï¼‰
- ALiBiï¼ˆAttention with Linear Biasesï¼‰

### 6.2 æ€§èƒ½ä¼˜åŒ–

**æ–° GPU æ¶æ„æ”¯æŒ**ï¼š
- Hopper (H100)ï¼šä¼˜åŒ– FP8 æ”¯æŒï¼Œåˆ©ç”¨ Transformer Engine
- Ada Lovelace (RTX 4090)ï¼šä¼˜åŒ–æ—¶é’Ÿé¢‘ç‡å’Œå†…å­˜å¸¦å®½
- Grace (CPU+GPU)ï¼šä¼˜åŒ–ç»Ÿä¸€å†…å­˜æ¶æ„

**æ··åˆç²¾åº¦**ï¼š
- FP8 æ”¯æŒï¼šåˆ©ç”¨ H100 FP8 Tensor Core
- BF16 ä¼˜åŒ–ï¼šæ›´å¥½çš„æ•°å€¼ç¨³å®šæ€§
- INT8 é‡åŒ–ï¼šæ¨ç†åŠ é€Ÿï¼ˆéœ€æ”¯æŒé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼‰

### 6.3 åˆ†å¸ƒå¼æ”¯æŒ

**å¤š GPU é€šä¿¡**ï¼š
- all_reduce, all_gather, reduce_scatter
- NCCL é›†æˆï¼šä¼˜åŒ– GPU é—´é€šä¿¡

**å¼ é‡å¹¶è¡Œ**ï¼š
- åˆ†å—çŸ©é˜µä¹˜æ³•ï¼ˆColumnParallel / RowParallelï¼‰
- åŒæ­¥åŸè¯­ï¼šbarrier, broadcast

**æµæ°´çº¿å¹¶è¡Œ**ï¼š
- å¾®æ‰¹æ¬¡è°ƒåº¦
- æ¢¯åº¦ç´¯ç§¯

### 6.4 æ˜“ç”¨æ€§æ”¹è¿›

**åŠ¨æ€å½¢çŠ¶æ”¯æŒ**ï¼š
- ç›®å‰é™åˆ¶ï¼šå¼ é‡ç»´åº¦å¿…é¡»åœ¨ç¼–è¯‘æ—¶ç¡®å®š
- æ”¹è¿›æ–¹æ¡ˆï¼šç¬¦å·åŒ–å½¢çŠ¶ + è¿è¡Œæ—¶åˆ†å‘

**è°ƒè¯•å·¥å…·**ï¼š
- å¯è§†åŒ–å†…å­˜å¸ƒå±€ï¼švisualization.visualize(tensor)
- æ€§èƒ½åˆ†æå™¨ï¼šè®°å½•å†…æ ¸æ‰§è¡Œæ—¶é—´ã€å†…å­˜å¸¦å®½ã€ç¼“å­˜å‘½ä¸­ç‡
- ç¬¦å·æ±‚å€¼ï¼ševal(tensor, {block_size: 64})

**æ–‡æ¡£ä¸æ•™ç¨‹**ï¼š
- API å‚è€ƒæ–‡æ¡£
- æ€§èƒ½è°ƒä¼˜æŒ‡å—
- æœ€ä½³å®è·µæ¡ˆä¾‹

## 7. æ€»ç»“

ntops ä½œä¸º Infini ç”Ÿæ€ç³»ç»Ÿçš„ç®—å­åŠ é€Ÿå±‚ï¼Œé€šè¿‡ ninetoothed ç¼–è¯‘å™¨æ¡†æ¶å’Œ PyTorch ç»‘å®šï¼Œå®ç°äº†é«˜æ€§èƒ½ã€æ˜“ç”¨æ€§ã€å¯æ‰©å±•æ€§çš„å¹³è¡¡ã€‚å…¶æ ¸å¿ƒä¼˜åŠ¿åŒ…æ‹¬ï¼š

1. **åŒå±‚æ¶æ„è®¾è®¡**ï¼škernels å±‚å®ç°è®¡ç®—é€»è¾‘ï¼Œtorch å±‚æä¾›ç”¨æˆ·æ¥å£ï¼ŒèŒè´£æ¸…æ™°
2. **ç¼–è¯‘æ—¶ä¼˜åŒ–**ï¼šåˆ©ç”¨ ninetoothed çš„ç¬¦å·åŒ–å¼ é‡æŠ½è±¡ï¼Œå®ç°é«˜åº¦ä¼˜åŒ–çš„å†…å­˜åˆ†å—å’Œå¹¶è¡Œç­–ç•¥
3. **è‡ªåŠ¨è°ƒä¼˜**ï¼šæ ¹æ®ç¡¬ä»¶ç‰¹æ€§è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å†…æ ¸é…ç½®
4. **é«˜çº§ç‰¹æ€§æ”¯æŒ**ï¼šåŸç”Ÿæ”¯æŒ Flash Attentionã€GQAã€KV ç¼“å­˜ã€RoPE ç­‰ç°ä»£ LLM æ ¸å¿ƒç®—å­
5. **å®Œå…¨å…¼å®¹ PyTorch**ï¼šå¯ç›´æ¥æ›¿æ¢ PyTorch åŸç”Ÿç®—å­ï¼Œæ— éœ€ä¿®æ”¹ç”¨æˆ·ä»£ç 

é€šè¿‡æŒç»­ä¼˜åŒ–å’Œæ‰©å±•ï¼Œntops å°†ä¸º Infini ç”Ÿæ€ç³»ç»Ÿæä¾›æ›´å¼ºçš„ç®—åŠ›æ”¯æŒï¼Œæ¨åŠ¨å¤§æ¨¡å‹è®­ç»ƒå’Œæ¨ç†çš„æ€§èƒ½è¾¹ç•Œã€‚

---

**ç›¸å…³æ–‡æ¡£**ï¼š
- [ninetoothed ç¼–è¯‘å™¨æ–‡æ¡£](../ninetoothed/README_ANALYSIS.md)
- [ntops API å‚è€ƒ](https://github.com/InfiniTensor/ntops)
- [InfiniLM æ¡†æ¶æ–‡æ¡£](../InfiniLM/README_ANALYSIS.md)

**æœ€åæ›´æ–°**ï¼š2025-01-14
