# `InfiniLM C++ Extension Loader` Core Implementation Documentation

The `lib` directory serves as the Python-C++ binding bridge for InfiniLM, responsible for loading and exposing the compiled `_infinilm` C++ extension module to Python. This module provides high-performance inference capabilities for large language models through native C++ implementations.

## 1. Module Structure

- **`__init__.py`**: Module initialization file that manages sys.path manipulation and imports the compiled `_infinilm` shared library
- **`_infinilm.so`** (compiled): The compiled C++ extension module containing all core inference functionality (generated by xmake build system)

## 2. Core Components

### Module Import System
- **Location**: `__init__.py`
- **Primary Function**: Dynamically configures Python's import path to locate and import the compiled C++ extension module
- **Key Operations**:
  - **Path Resolution**: Uses `Path(__file__).parent` to locate the directory containing the `__init__.py` file
  - **Path Injection**: Inserts the library directory into `sys.path` at position 0 to ensure the `.so` file is discoverable
  - **Module Import**: Imports the compiled `_infinilm` module (shared library built by xmake)
- **Lifecycle**:
  - **Initialization**: Executes when `infinilm.lib` is first imported
  - **Path Management**: Ensures the compiled extension is importable regardless of current working directory
  - **Export**: Exposes `_infinilm` through `__all__` for explicit public API

## 3. API Interface

The `lib` module exposes the following C++-bound classes and functions through `_infinilm`:

### Core Inference Engine
```python
from infinilm.lib import _infinilm

# Primary inference engine class
class InferEngine:
    def __init__(
        self,
        config: LlamaConfig,
        distributed_config: DistConfig = DistConfig(),
        device_type: infinicore.Device.Type = infinicore.context.getDevice().getType(),
        cache_config: Optional[CacheConfig] = None
    )
    # Initializes the inference engine with model configuration, distributed settings,
    # device type (CPU/CUDA/etc.), and optional KV cache configuration

    def load_param(self, name: str, param: infinicore.Tensor) -> None
    # Loads a model parameter (weight/bias) into all workers, with automatic
    # tensor parallel sharding across devices

    def state_dict(self) -> List[Dict[str, infinicore.Tensor]]
    # Returns all model parameters organized by tensor parallel rank
    # Each dict contains parameter_name -> tensor mappings for one TP rank

    def forward(self, input: Input) -> Output
    # Executes a single forward pass through the model
    # Input contains: input_ids, position_ids, cache_lengths, block_tables, etc.
    # Output contains: output_ids tensor

    def reset_cache(self, cache_config: Optional[CacheConfig] = None) -> None
    # Resets or reconfigures the KV cache with new settings
```

### Model Configuration
```python
class LlamaConfig:
    vocab_size: int              # Size of vocabulary
    hidden_size: int             # Dimension of hidden states
    intermediate_size: int       # Dimension of MLP intermediate layer
    num_hidden_layers: int       # Number of transformer layers
    num_attention_heads: int     # Number of attention heads
    num_key_value_heads: int     # Number of KV heads (for GQA)
    head_dim: int                # Dimension per attention head
    max_position_embeddings: int # Maximum sequence length
    rms_norm_eps: float          # RMS normalization epsilon
    hidden_act: str              # Activation function (e.g., "silu")
    rope_theta: float            # RoPE rotation theta
    attention_bias: bool         # Whether attention uses bias
    tie_word_embeddings: bool    # Whether to tie input/output embeddings
    use_cache: bool              # Whether to use KV cache
    eos_token_id: List[int]      # End-of-sequence token IDs
    bos_token_id: List[int]      # Beginning-of-sequence token IDs

    def validate(self) -> None
    # Validates configuration parameters for consistency

    def kv_dim(self) -> int
    # Returns the dimension of key/value vectors
```

### Distributed Configuration
```python
class DistConfig:
    def __init__()  # Default: single device, no parallelism
    def __init__(tp_size: int)  # Auto-assign devices 0..tp_size-1
    def __init__(tp_device_ids: List[int])  # Explicit device assignment

    tp_device_ids: List[int]
    # List of device IDs for tensor parallelism
    # e.g., [0, 1, 2, 3] for 4-way TP on GPUs 0-3
```

### KV Cache Configuration
```python
# Abstract base class
class CacheConfig:
    pass

# Static cache with fixed size allocation
class StaticKVCacheConfig(CacheConfig):
    def __init__(self, max_batch_size: int = 1, max_cache_len: int = 2^63-1)
    # Allocates contiguous memory for KV cache
    # max_batch_size: maximum batch size
    # max_cache_len: maximum sequence length per batch

    def max_batch_size(self) -> int
    def max_cache_len(self) -> int

# Paged cache with dynamic memory management
class PagedKVCacheConfig(CacheConfig):
    def __init__(self, max_kv_memory_bytes: int, block_size: int = 16)
    # Uses paged memory allocator for KV cache
    # max_kv_memory_bytes: total memory budget for KV storage
    # block_size: number of tokens per cache block

    def max_kv_memory_bytes(self) -> int
    def block_size(self) -> int
```

### Inference Input/Output
```python
class InferEngine.Input:
    input_ids: Optional[infinicore.Tensor]        # Token IDs [batch, seq_len]
    position_ids: Optional[infinicore.Tensor]     # Position IDs [batch, seq_len]
    cache_lengths: Optional[infinicore.Tensor]    # Cached sequence lengths [batch]
    input_lengths: Optional[infinicore.Tensor]    # Input sequence lengths [batch]
    block_tables: Optional[infinicore.Tensor]     # Physical block mappings [batch, max_blocks]
    slot_mapping: Optional[infinicore.Tensor]     # Cache slot mappings [total_tokens]
    temperature: Optional[float] = 1.0            # Sampling temperature
    top_k: Optional[int] = 50                     # Top-k sampling parameter
    top_p: Optional[float] = 1.0                  # Top-p (nucleus) sampling parameter

class InferEngine.Output:
    output_ids: infinicore.Tensor  # Generated token IDs [batch, 1]
```

### Debug Hooks
```python
class HookRegistry:
    def __init__(self)
    # Creates a registry for debugging hooks

    def register_hook(self, name: str, callback: Callable) -> None
    # Registers a callback function to be invoked during model execution
    # Callback signature: (hook_name: str, tensor: infinicore.Tensor, layer_idx: int)

    def clear(self) -> None
    # Removes all registered hooks

    def has_hooks(self) -> bool
    # Returns True if any hooks are registered
```

## 4. Usage Example

```python
import infinicore
from infinilm.lib import _infinilm
from infinilm.auto_config import AutoConfig

# 1. Load model configuration
config = AutoConfig.from_pretrained("/path/to/model")
# This returns a _infinilm.LlamaConfig with all parameters loaded

# 2. Configure distributed inference
dist_config = _infinilm.DistConfig(tp_size=4)  # 4-way tensor parallelism

# 3. Configure device
device = infinicore.device("nvidia", 0)

# 4. Configure KV cache (optional)
cache_config = _infinilm.StaticKVCacheConfig(
    max_batch_size=16,
    max_cache_len=4096
)

# 5. Create inference engine
engine = _infinilm.InferEngine(
    config=config,
    distributed_config=dist_config,
    device_type=device._underlying.type,
    cache_config=cache_config
)

# 6. Load model weights
state_dict = get_model_state_dict(model_path)  # Your weight loading logic
for name, param in state_dict.items():
    engine.load_param(name, param._underlying)

# 7. Prepare input tensors
input_ids = infinicore.from_list([[1, 2, 3, 4]], dtype=infinicore.int64)
position_ids = infinicore.from_list([[0, 1, 2, 3]], dtype=infinicore.int64)
cache_lengths = infinicore.from_list([0], dtype=infinicore.int64)

# 8. Run inference
input_struct = _infinilm.InferEngine.Input(
    input_ids=input_ids._underlying,
    position_ids=position_ids._underlying,
    cache_lengths=cache_lengths._underlying,
    temperature=0.8,
    top_k=50,
    top_p=0.95
)

output = engine.forward(input_struct)
generated_token = output.output_ids  # infinicore.Tensor [batch, 1]

# 9. Reset cache (optional)
engine.reset_cache(cache_config)
```

## 5. Implementation Details

### Build System Integration
- **Compiler**: xmake-based build system
- **Language**: C++17 standard
- **Binding Framework**: pybind11 v2.x+
- **Output**: Shared library (`_infinilm.so`) installed to `python/infinilm/lib/`

### C++ Backend Architecture
The compiled module binds to the following C++ components:

**Core C++ Classes** (defined in `/home/qy/src/Infini/InfiniLM/csrc/`):
- `InfiniLM::engine::InferEngine`: Main inference orchestrator managing rank workers
- `InfiniLM::models::llama::LlamaConfig`: LLaMA model configuration
- `InfiniLM::engine::distributed::DistConfig`: Tensor parallelism configuration
- `InfiniLM::cache::StaticKVCacheConfig`: Static KV cache allocator
- `InfiniLM::cache::PagedKVCacheConfig`: Paged KV cache allocator
- `InfiniLM::models::debug_utils::HookRegistry`: Debugging hook system

**Memory Management**:
- Uses `std::shared_ptr` for automatic lifecycle management
- InfiniCore tensors manage device memory through RAII patterns
- KV cache supports both static allocation and paged memory management

**Concurrency Model**:
- Multi-process distributed inference via MPI-style communication
- Tensor parallelism with column/row linear sharding
- Each rank runs in a separate process with dedicated device ownership
- Communication through InfiniCCL (collective communications library)

**Performance Optimizations**:
- Fused kernels for attention and MLP computations
- Custom CUDA/HIP/BANG kernels for various hardware backends
- KV cache paged attention for variable-length sequence handling
- Flash attention implementation for memory-efficient attention

### Dependencies
**Runtime Dependencies**:
- `InfiniCore`: Core tensor operations and device abstraction
- `InfiniRT`: Runtime and device management
- `InfiniOP`: Operator implementations (GEMM, attention, etc.)
- `InfiniCCL`: Collective communications for distributed training

**Build Dependencies**:
- `pybind11`: Python-C++ binding generation
- `spdlog`: C++ logging framework (header-only)
- Hardware SDKs: CUDA, ROCm, BANG, CANN, MUSA, etc.

### Design Patterns
- **Wrapper Pattern**: Python classes wrap C++ objects via `_underlying` attribute
- **Factory Pattern**: `InferEngine` constructs specialized model implementations
- **Strategy Pattern**: Pluggable cache implementations (Static vs Paged)
- **Observer Pattern**: HookRegistry for debugging and monitoring
- **Shared Ownership**: `std::shared_ptr` for cross-language object sharing

### Thread Safety
- **Python GIL**: pybind11 automatically manages GIL acquisition/release
- **C++ Side**: Thread-safe for read-only operations after initialization
- **Device Operations**: Each process owns exclusive device access
- **State Mutation**: `load_param` and `reset_cache` require external synchronization

### Error Handling
- **C++ Exceptions**: Automatically converted to Python exceptions by pybind11
- **Validation**: `LlamaConfig.validate()` checks parameter consistency
- **Memory Errors**: Out-of-memory conditions raise `std::bad_alloc`
- **Device Errors**: Hardware failures propagate through InfiniCore error handling

### Hardware Support
The extension module supports multiple hardware backends through InfiniCore abstraction:
- **NVIDIA**: CUDA compute capability
- **AMD**: ROCm/HIP support
- **Cambricon**: BANG language (寒武纪)
- **Huawei**: CANN framework (昇腾)
- **Moore Threads**: MUSA toolkit (摩尔线程)
- **Iluvatar**: MetaX (天数智芯)
- **Kunlun**: KUNLUN computing (昆仑)
- **Hygon**: DCU (海光)

Device selection is transparent through `infinicore.device()` abstraction.
