#!/usr/bin/env python3
"""
Fusion Strategy End-to-End Comparison

ç«¯åˆ°ç«¯æ¨ç†å¯¹æ¯”æµ‹è¯•ï¼š
1. always_fuse: å§‹ç»ˆèåˆ
2. never_fuse: å§‹ç»ˆä¸èåˆ  
3. smart_schedule: æ™ºèƒ½è°ƒåº¦ (åŸºäº profile å†³ç­–)

Usage:
    python examples/benchmark_fusion_e2e.py \
        --iluvatar \
        --model_path /data/liuxingyu/OpCompiler/TinyLlama-1.1B-Chat-v1.0 \
        --prompt "What is the capital of France?" \
        --max_new_tokens 50 \
        --runs 3
"""

import infinicore
from transformers import AutoTokenizer
from tokenizers import decoders as _dec
from infinilm.modeling_utils import load_model_state_dict_by_file
from infinilm.distributed import DistConfig
from infinilm.infer_engine import GenerationConfig, InferEngine
from infinilm.fused_infer_engine import FusedInferEngine
import argparse
import sys
import time
import os
import numpy as np

sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../python"))


def get_args():
    parser = argparse.ArgumentParser(description="Fusion Strategy E2E Comparison")
    
    # Device
    parser.add_argument("--cpu", action="store_true", help="Run on CPU")
    parser.add_argument("--nvidia", action="store_true", help="Run on NVIDIA GPU")
    parser.add_argument("--iluvatar", action="store_true", help="Run on ILUVATAR GPU")
    parser.add_argument("--metax", action="store_true", help="Run on MetaX")
    parser.add_argument("--moore", action="store_true", help="Run on Moore")
    
    # Model
    parser.add_argument("--model_path", type=str, required=True)
    parser.add_argument("--tp", type=int, default=1, help="Tensor parallelism")
    
    # Generation
    parser.add_argument("--max_new_tokens", type=int, default=50)
    
    # Benchmark
    parser.add_argument("--runs", type=int, default=2, help="Number of runs per prompt")
    parser.add_argument("--warmup", type=int, default=1, help="Warmup runs")
    
    return parser.parse_args()


# ============================================================
# æµ‹è¯• Prompts - ä¸åŒé•¿åº¦ä½“ç°ä¸åŒåœºæ™¯
# 
# predicted_best: ç†è®ºé¢„æµ‹å“ªç§ç­–ç•¥æ›´ä¼˜
#   - "never_fuse": çŸ­åºåˆ—ï¼Œkernel launch å¼€é”€å æ¯”å¤§ï¼Œèåˆåè€Œå¢åŠ å¼€é”€
#   - "always_fuse": é•¿åºåˆ—ï¼Œmemory-boundï¼Œèåˆå‡å°‘å†…å­˜è®¿é—®
#   - "smart_schedule": æ™ºèƒ½é€‰æ‹©ï¼ˆåº”è¯¥å’Œ always_fuse æˆ– never_fuse æ¥è¿‘ï¼‰
#
# ç†è®ºä¾æ®ï¼š
#   - Prefill é˜¶æ®µ: å¤„ç†é•¿åºåˆ—ï¼Œå¤§ shapeï¼Œèåˆæ›´æœ‰åˆ©ï¼ˆå‡å°‘å†…å­˜å¸¦å®½ï¼‰
#   - Decode é˜¶æ®µ: æ¯æ¬¡å¤„ç† 1 ä¸ª tokenï¼Œå° shapeï¼Œèåˆå¼€é”€å¯èƒ½æ›´å¤§
# ============================================================
TEST_PROMPTS = [
    # ========== çŸ­ Prompt (decode ä¸ºä¸») ==========
    # ç†è®º: çŸ­è¾“å…¥ + çŸ­è¾“å‡º = å¤§éƒ¨åˆ†æ—¶é—´åœ¨ decode (seq_len=1)
    # é¢„æµ‹: never_fuse æ›´å¥½ï¼ˆå° shape æ—¶ kernel launch å¼€é”€æ˜¾è‘—ï¼‰
    {
        "name": "short_qa",
        "prompt": "Hi",
        "max_tokens": 20,
        "category": "decode_heavy",
        "description": "æçŸ­è¾“å…¥ï¼Œä¸»è¦æµ‹ decode æ€§èƒ½",
        "predicted_best": "never_fuse",
        "reason": "æçŸ­åºåˆ—ï¼Œdecode å ä¸»å¯¼ï¼Œèåˆå¼€é”€ > æ”¶ç›Š",
    },
    {
        "name": "simple_question",
        "prompt": "What is 2+2?",
        "max_tokens": 30,
        "category": "decode_heavy",
        "description": "ç®€å•é—®é¢˜ï¼ŒçŸ­è¾“å…¥çŸ­è¾“å‡º",
        "predicted_best": "never_fuse",
        "reason": "çŸ­åºåˆ—ï¼Œkernel launch å¼€é”€æ˜¾è‘—",
    },
    
    # ========== ä¸­ç­‰ Prompt ==========
    # ç†è®º: ä¸­ç­‰é•¿åº¦ï¼Œprefill å’Œ decode æ—¶é—´ç›¸å½“
    # é¢„æµ‹: smart_schedule æˆ– always_fuse ç•¥å¥½
    {
        "name": "medium_qa",
        "prompt": "Explain the concept of machine learning in simple terms.",
        "max_tokens": 100,
        "category": "balanced",
        "description": "ä¸­ç­‰é•¿åº¦é—®ç­”",
        "predicted_best": "always_fuse",
        "reason": "ä¸­ç­‰é•¿åº¦ï¼Œprefill å æ¯”ä¸Šå‡ï¼Œèåˆå¼€å§‹æœ‰æ”¶ç›Š",
    },
    {
        "name": "code_request",
        "prompt": "Write a Python function to calculate fibonacci numbers.",
        "max_tokens": 150,
        "category": "balanced",
        "description": "ä»£ç ç”Ÿæˆè¯·æ±‚",
        "predicted_best": "always_fuse",
        "reason": "è¾ƒé•¿è¾“å‡ºï¼Œdecode å¤šä½† prefill ä¹Ÿæœ‰ä¸€å®šæ¯”ä¾‹",
    },
    
    # ========== é•¿ Prompt (prefill ä¸ºä¸») ==========
    # ç†è®º: é•¿è¾“å…¥ = prefill æ—¶é—´é•¿ï¼Œå¤„ç†å¤§ shapeï¼Œèåˆæ”¶ç›Šæ˜æ˜¾
    # é¢„æµ‹: always_fuse æ˜æ˜¾æ›´å¥½
    {
        "name": "long_context",
        "prompt": """Here is a story: Once upon a time, in a small village nestled between rolling hills and a sparkling river, there lived a young girl named Aria. She was known throughout the village for her curiosity and kind heart. Every morning, she would wake before dawn to help her grandmother tend to their small garden of herbs and vegetables.

One day, while exploring the forest beyond the village, Aria discovered a hidden path she had never seen before. The path was lined with glowing mushrooms and led deep into the woods. What should Aria do next?""",
        "max_tokens": 100,
        "category": "prefill_heavy",
        "description": "é•¿ä¸Šä¸‹æ–‡ï¼Œä¸»è¦æµ‹ prefill æ€§èƒ½",
        "predicted_best": "always_fuse",
        "reason": "é•¿è¾“å…¥ (~120 tokens)ï¼Œprefill å æ¯”é«˜ï¼Œèåˆå‡å°‘å†…å­˜è®¿é—®",
    },
    {
        "name": "summarization",
        "prompt": """Please summarize the following text:

Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans. AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals. The term "artificial intelligence" had previously been used to describe machines that mimic and display "human" cognitive skills that are associated with the human mind, such as "learning" and "problem-solving". This definition has since been rejected by major AI researchers who now describe AI in terms of rationality and acting rationally.

Summary:""",
        "max_tokens": 80,
        "category": "prefill_heavy",
        "description": "æ‘˜è¦ä»»åŠ¡ï¼Œé•¿è¾“å…¥ä¸­ç­‰è¾“å‡º",
        "predicted_best": "always_fuse",
        "reason": "é•¿è¾“å…¥ (~100 tokens)ï¼Œprefill ä¸»å¯¼ï¼Œèåˆæ”¶ç›Šå¤§",
    },
    
    # ========== æçŸ­è¯·æ±‚ ==========
    # ç†è®º: æçŸ­åºåˆ—ï¼Œå‡ ä¹åªæœ‰ decode
    # é¢„æµ‹: never_fuse æ›´å¥½
    {
        "name": "batch_short",
        "prompt": "Hello!",
        "max_tokens": 10,
        "category": "decode_heavy",
        "description": "æçŸ­è¯·æ±‚ï¼Œæµ‹è¯• kernel launch å¼€é”€",
        "predicted_best": "never_fuse",
        "reason": "æçŸ­åºåˆ—ï¼Œèåˆé¢å¤–å¼€é”€ > æ”¶ç›Š",
    },
]


def run_inference(
    model,
    tokenizer,
    prompt: str,
    max_new_tokens: int,
    device,
) -> tuple:
    """
    è¿è¡Œä¸€æ¬¡æ¨ç† (ä½¿ç”¨æ‰‹åŠ¨ Python å±‚ sampling å¾ªç¯æ¥ç»•è¿‡ C++ random_sample bug)

    Returns:
        (output_text, time_ms)
    """
    # Tokenize
    input_content = tokenizer.apply_chat_template(
        conversation=[{"role": "user", "content": prompt}],
        add_generation_prompt=True,
        tokenize=False,
    )
    # Fix: use encode() instead of batch_encode_plus() for newer transformers versions
    input_ids_list = [tokenizer.encode(input_content)]

    # Reset cache
    model.reset_cache(1, max_new_tokens + len(input_ids_list[0]))

    input_ids_infini = infinicore.from_list(input_ids_list, device=device)

    # é™é»˜è¾“å‡º (é€šè¿‡é‡å®šå‘ stdout)
    import io
    import sys
    old_stdout = sys.stdout
    sys.stdout = io.StringIO()

    start = time.perf_counter()
    try:
        # æ‰‹åŠ¨å®ç° generation å¾ªç¯ï¼Œä½¿ç”¨ Python å±‚ sampling ç»•è¿‡ C++ random_sample bug
        batch_size, seq_len = input_ids_infini.shape[:2]

        # åˆå§‹åŒ– position_ids å’Œ cache_lengths
        position_ids = infinicore.from_list(
            [list(range(0, seq_len)) for _ in range(batch_size)],
            dtype=infinicore.int64,
            device=device,
        )
        cache_lengths = infinicore.from_list(
            [0],
            dtype=infinicore.int64,
            device=device,
        )

        output_tokens_list = []
        eos_token_id = model.config.eos_token_id
        eos_token_id_list = [eos_token_id] if isinstance(eos_token_id, int) else eos_token_id

        for _ in range(max_new_tokens):
            # è°ƒç”¨ forward è·å– logits (ä¸ä¼ é€’é‡‡æ ·å‚æ•°ï¼Œé¿å…è§¦å‘ C++ é‡‡æ ·)
            logits = model(
                input_ids=input_ids_infini,
                position_ids=position_ids,
                cache_lengths=cache_lengths,
            )
            infinicore.sync_device()

            # Python å±‚ greedy decoding (ä½¿ç”¨ argmax)
            # Convert logits to numpy and do argmax there (infinicore doesn't have argmax)
            logits_np = logits.to_numpy()
            next_token_id = int(logits_np.argmax(axis=-1)[0, 0])
            token_id = next_token_id
            output_tokens_list.append(token_id)

            # æ£€æŸ¥ EOS
            if token_id in eos_token_id_list:
                break

            # å‡†å¤‡ä¸‹ä¸€è½®è¾“å…¥
            seq_len = position_ids.shape[-1]
            input_ids_infini = infinicore.from_list(
                [[token_id] for _ in range(batch_size)],
                dtype=infinicore.int64,
                device=device,
            )
            position_ids = infinicore.from_list(
                [1] * batch_size,
                dtype=infinicore.int64,
                device=device,
            ).view((batch_size, 1)) + position_ids.narrow(1, seq_len - 1, 1)
            cache_lengths = cache_lengths + infinicore.from_list(
                [seq_len],
                dtype=infinicore.int64,
                device=device,
            )

        # è§£ç è¾“å‡º
        output_text = tokenizer.decode(output_tokens_list, skip_special_tokens=True)
        print(output_text, end="", flush=True)
    finally:
        output_text = sys.stdout.getvalue()
        sys.stdout = old_stdout
    
    end = time.perf_counter()
    time_ms = (end - start) * 1000.0
    
    return output_text.strip(), time_ms


def load_model_with_strategy(
    model_path: str,
    device,
    tp: int,
    strategy: str,
    profile_path: str = None,
    debug: bool = False,
) -> tuple:
    """
    æ ¹æ®ç­–ç•¥åŠ è½½æ¨¡å‹ (ä½¿ç”¨ C++ infiniop èåˆåç«¯)
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        device: è®¾å¤‡
        tp: å¼ é‡å¹¶è¡Œåº¦
        strategy: ç­–ç•¥ - "always_fuse" | "never_fuse" | "smart_schedule"
        profile_path: profile æ•°æ®è·¯å¾„ (ä»… smart_schedule æ—¶ä½¿ç”¨)
        debug: æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯
    """
    model_path = os.path.expanduser(model_path)
    
    if strategy == "always_fuse":
        # ä½¿ç”¨ FusedInferEngineï¼Œå§‹ç»ˆèåˆ
        model = FusedInferEngine(
            model_path,
            device=device,
            distributed_config=DistConfig(tp),
            enable_fusion=True,
            fusion_mode="always",
            debug=debug,
        )
        
    elif strategy == "never_fuse":
        # ä½¿ç”¨æ™®é€š InferEngineï¼Œä¸èåˆ
        model = InferEngine(
            model_path,
            device=device,
            distributed_config=DistConfig(tp),
        )
        
    elif strategy == "smart_schedule":
        # ä½¿ç”¨ FusedInferEngineï¼ŒåŸºäº profile æ™ºèƒ½è°ƒåº¦
        model = FusedInferEngine(
            model_path,
            device=device,
            distributed_config=DistConfig(tp),
            enable_fusion=True,
            fusion_mode="profile",
            profile_path=profile_path,
            debug=debug,
        )
        
    else:
        raise ValueError(f"Unknown strategy: {strategy}")
    
    # åŠ è½½æƒé‡
    load_model_state_dict_by_file(model, model_path, dtype=model.config.dtype)
    
    # åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    # ä¿®å¤ LLaMA tokenizer
    if getattr(model.config, "model_type", "") == "llama":
        backend = getattr(tokenizer, "backend_tokenizer", None)
        target = getattr(backend, "_tokenizer", backend)
        norm = getattr(target, "normalizer", None)
        dec = getattr(target, "decoder", None)
        sn = repr(norm)[:800] if norm is not None else ""
        sd = repr(dec)[:800] if dec is not None else ""
        has_prepend = "Prepend" in sn
        has_strip = "Strip" in sd
        if has_prepend and has_strip:
            target.decoder = _dec.Sequence([
                _dec.Replace("â–", " "),
                _dec.ByteFallback(),
                _dec.Fuse(),
            ])
    
    return model, tokenizer


def benchmark_strategy(
    model_path: str,
    device,
    tp: int,
    prompt: str,
    max_new_tokens: int,
    strategy: str,
    runs: int,
    warmup: int,
) -> dict:
    """
    å¯¹å•ä¸ªç­–ç•¥è¿›è¡Œå¤šæ¬¡æµ‹è¯•
    """
    print(f"\n{'='*60}")
    print(f"Strategy: {strategy}")
    print(f"{'='*60}")
    
    # åŠ è½½æ¨¡å‹
    print(f"Loading model...")
    model, tokenizer = load_model_with_strategy(model_path, device, tp, strategy)
    
    times = []
    
    # Warmup
    print(f"Warmup ({warmup} runs)...")
    for i in range(warmup):
        _, _ = run_inference(model, tokenizer, prompt, max_new_tokens, device)
    
    # Timed runs
    print(f"Benchmark ({runs} runs)...")
    for i in range(runs):
        output_text, time_ms = run_inference(model, tokenizer, prompt, max_new_tokens, device)
        times.append(time_ms)
        print(f"  Run {i+1}: {time_ms:.2f} ms")
    
    # Show sample output
    print(f"Sample output: {output_text[:100]}...")
    
    avg_time = sum(times) / len(times)
    min_time = min(times)
    max_time = max(times)
    
    print(f"Results: avg={avg_time:.2f}ms, min={min_time:.2f}ms, max={max_time:.2f}ms")
    
    # è·å–èåˆç»Ÿè®¡ï¼ˆå¦‚æœæœ‰ï¼‰
    fusion_stats = None
    if hasattr(model, 'get_stats'):
        fusion_stats = model.get_stats()
        print(f"Fusion stats: {fusion_stats}")
    
    return {
        "strategy": strategy,
        "times": times,
        "avg_time": avg_time,
        "min_time": min_time,
        "max_time": max_time,
        "fusion_stats": fusion_stats,
    }


def run_all_prompts_with_strategy(
    model,
    tokenizer,
    prompts: list,
    runs: int,
    warmup: int,
    device,  # æ·»åŠ  device å‚æ•°
) -> dict:
    """å¯¹ä¸€ä¸ªç­–ç•¥è¿è¡Œæ‰€æœ‰ prompts"""
    results = {}
    
    for p in prompts:
        name = p["name"]
        prompt = p["prompt"]
        max_tokens = p["max_tokens"]
        
        times = []
        
        # Warmup
        for _ in range(warmup):
            run_inference(model, tokenizer, prompt, max_tokens, device)
        
        # Timed runs
        for _ in range(runs):
            _, time_ms = run_inference(model, tokenizer, prompt, max_tokens, device)
            times.append(time_ms)
        
        avg_time = sum(times) / len(times)
        results[name] = {
            "avg_time": avg_time,
            "times": times,
            "category": p["category"],
            "description": p["description"],
        }
    
    return results


def main():
    args = get_args()
    
    # ç¡®å®šè®¾å¤‡
    if args.nvidia:
        device_str = "cuda"
    elif args.iluvatar:
        device_str = "cuda"  # ILUVATAR ä½¿ç”¨ cuda æ¥å£
    elif args.cpu:
        device_str = "cpu"
    elif args.metax:
        device_str = "maca"
    elif args.moore:
        device_str = "musa"
    else:
        print("Please specify device: --cpu, --nvidia, --iluvatar, --metax, or --moore")
        sys.exit(1)
    
    device = infinicore.device(device_str, 0)
    
    print("=" * 80)
    print("Fusion Strategy E2E Comparison - Multi-Prompt Benchmark")
    print("=" * 80)
    print(f"Device: {device_str}")
    print(f"Model: {args.model_path}")
    print(f"Runs per prompt: {args.runs}, Warmup: {args.warmup}")
    print(f"Test prompts: {len(TEST_PROMPTS)}")
    
    # æµ‹è¯•ä¸‰ç§ç­–ç•¥
    strategies = ["never_fuse", "always_fuse", "smart_schedule"]
    all_results = {}
    
    for strategy in strategies:
        print(f"\n{'='*80}")
        print(f"ğŸ“Œ Strategy: {strategy}")
        print(f"{'='*80}")
        
        try:
            print("Loading model...")
            model, tokenizer = load_model_with_strategy(
                args.model_path, device, args.tp, strategy
            )
            
            print(f"Running {len(TEST_PROMPTS)} prompts...")
            results = run_all_prompts_with_strategy(
                model, tokenizer, TEST_PROMPTS, args.runs, args.warmup, device
            )
            
            all_results[strategy] = results
            
            # æ˜¾ç¤ºè¯¥ç­–ç•¥çš„ç»“æœ
            print(f"\n{'Prompt':<20} {'Category':<15} {'Avg Time (ms)':<15}")
            print("-" * 55)
            for name, r in results.items():
                print(f"{name:<20} {r['category']:<15} {r['avg_time']:<15.2f}")
            
            total = sum(r["avg_time"] for r in results.values())
            print(f"{'TOTAL':<20} {'':<15} {total:<15.2f}")
            
        except Exception as e:
            print(f"ERROR: {e}")
            import traceback
            traceback.print_exc()
            all_results[strategy] = {"error": str(e)}
    
    # ========== Detailed Comparison ==========
    print("\n" + "=" * 80)
    print("ğŸ“Š PER-PROMPT COMPARISON")
    print("=" * 80)
    
    valid_strategies = [s for s in strategies if "error" not in all_results.get(s, {})]
    
    if len(valid_strategies) >= 2:
        # Header
        header = f"{'Prompt':<20} {'Predicted':<12}"
        for s in valid_strategies:
            header += f" {s:<12}"
        header += " Actual      Match"
        print(header)
        print("-" * (36 + 12 * len(valid_strategies) + 20))
        
        prompt_winners = {"never_fuse": 0, "always_fuse": 0, "smart_schedule": 0}
        prediction_correct = 0
        prediction_total = 0
        
        for p in TEST_PROMPTS:
            name = p["name"]
            predicted = p.get("predicted_best", "unknown")
            
            row = f"{name:<20} {predicted:<12}"
            times = {}
            for s in valid_strategies:
                if name in all_results[s]:
                    t = all_results[s][name]["avg_time"]
                    times[s] = t
                    row += f" {t:<12.1f}"
                else:
                    row += f" {'N/A':<12}"
            
            if times:
                best = min(times, key=times.get)
                prompt_winners[best] = prompt_winners.get(best, 0) + 1
                
                # æ£€æŸ¥é¢„æµ‹æ˜¯å¦æ­£ç¡®
                match = "âœ…" if best == predicted else "âŒ"
                if predicted != "unknown":
                    prediction_total += 1
                    if best == predicted:
                        prediction_correct += 1
                
                row += f" {best:<12} {match}"
            
            print(row)
        
        # Totals
        print("-" * (36 + 12 * len(valid_strategies) + 20))
        row = f"{'TOTAL':<20} {'':<12}"
        totals = {}
        for s in valid_strategies:
            total = sum(all_results[s][p["name"]]["avg_time"] for p in TEST_PROMPTS if p["name"] in all_results[s])
            totals[s] = total
            row += f" {total:<12.1f}"
        
        best_total = min(totals, key=totals.get)
        row += f" {best_total:<12} â­"
        print(row)
        
        # Prediction Accuracy
        print("\n" + "=" * 80)
        print("ğŸ¯ PREDICTION ACCURACY")
        print("=" * 80)
        if prediction_total > 0:
            accuracy = 100 * prediction_correct / prediction_total
            print(f"\nCorrect: {prediction_correct}/{prediction_total} ({accuracy:.1f}%)")
            if accuracy >= 70:
                print("âœ… ç†è®ºé¢„æµ‹ä¸å®é™…ç»“æœåŸºæœ¬å»åˆï¼")
            elif accuracy >= 50:
                print("âš ï¸ ç†è®ºé¢„æµ‹éƒ¨åˆ†æ­£ç¡®ï¼Œéœ€è¦æ›´å¤š profiling æ•°æ®")
            else:
                print("âŒ ç†è®ºé¢„æµ‹ä¸å®é™…ä¸ç¬¦ï¼Œéœ€è¦é‡æ–°åˆ†æ")
        
        # Strategy Summary
        print("\n" + "=" * 80)
        print("ğŸ“ˆ STRATEGY SUMMARY")
        print("=" * 80)
        
        baseline = max(totals.values())
        print(f"\n{'Strategy':<20} {'Total (ms)':<15} {'Speedup':<10} {'Wins':<10}")
        print("-" * 60)
        for s in valid_strategies:
            speedup = baseline / totals[s] if totals[s] > 0 else 0
            wins = prompt_winners.get(s, 0)
            marker = "â­" if s == best_total else ""
            print(f"{s:<20} {totals[s]:<15.2f} {speedup:<10.2f}x {wins:<10} {marker}")
        
        # Category Analysis
        print("\n" + "=" * 80)
        print("ğŸ“Š CATEGORY ANALYSIS")
        print("=" * 80)
        
        categories = ["decode_heavy", "balanced", "prefill_heavy"]
        for cat in categories:
            cat_prompts = [p for p in TEST_PROMPTS if p["category"] == cat]
            if not cat_prompts:
                continue
            
            print(f"\nã€{cat}ã€‘({len(cat_prompts)} prompts)")
            cat_totals = {}
            for s in valid_strategies:
                total = sum(
                    all_results[s][p["name"]]["avg_time"] 
                    for p in cat_prompts 
                    if p["name"] in all_results[s]
                )
                cat_totals[s] = total
            
            cat_baseline = max(cat_totals.values())
            for s in valid_strategies:
                speedup = cat_baseline / cat_totals[s] if cat_totals[s] > 0 else 0
                best_marker = "â­" if cat_totals[s] == min(cat_totals.values()) else ""
                print(f"  {s:<18}: {cat_totals[s]:.2f}ms ({speedup:.2f}x) {best_marker}")
    
    print("\n" + "=" * 80)
    print("âœ… Benchmark Complete")
    print("=" * 80)


if __name__ == "__main__":
    main()


