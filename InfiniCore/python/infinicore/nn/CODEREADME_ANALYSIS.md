# Python NN æ¨¡å—æ¶æ„å…¨æ™¯

## 1. å­ç³»ç»ŸèŒè´£

æœ¬ç›®å½• `infinicore.nn` æ˜¯ InfiniCore æ·±åº¦å­¦ä¹ æ¨ç†æ¡†æ¶çš„ Python ç¥ç»ç½‘ç»œå±‚æŠ½è±¡ï¼Œæä¾›äº†ä¸ PyTorch å…¼å®¹çš„é«˜å±‚ API å°è£…ã€‚è¯¥å­ç³»ç»Ÿé‡‡ç”¨**åŒå±‚æ¶æ„è®¾è®¡**ï¼š

* **å‡½æ•°å¼å±‚ï¼ˆfunctionalï¼‰**ï¼šæä¾›æ— çŠ¶æ€çš„çº¯å‡½æ•°æ¥å£ï¼Œç›´æ¥è°ƒç”¨åº•å±‚ C++ ç®—å­å®ç°ï¼Œå®ç°æ ¸å¿ƒç¥ç»ç½‘ç»œåŸè¯­ï¼ˆå¦‚çº¿æ€§å˜æ¢ã€æ¿€æ´»å‡½æ•°ã€å½’ä¸€åŒ–ã€ä½ç½®ç¼–ç ç­‰ï¼‰ã€‚
* **æ¨¡å—å±‚ï¼ˆmodulesï¼‰**ï¼šæä¾›é¢å‘å¯¹è±¡çš„æœ‰çŠ¶æ€ç»„ä»¶å°è£…ï¼Œé€šè¿‡å‚æ•°ç®¡ç†ã€çŠ¶æ€åºåˆ—åŒ–å’Œæ¨¡å—ç»„åˆæœºåˆ¶ï¼Œæ„å»ºå¯å¤ç”¨çš„ç¥ç»ç½‘ç»œå±‚ï¼ˆå¦‚ Linearã€RMSNormã€Embeddingã€RoPE ç­‰ï¼‰ã€‚

è¿™ç§è®¾è®¡éµå¾ª PyTorch çš„ API è§„èŒƒï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿä½¿ç”¨ç†Ÿæ‚‰çš„æ¥å£å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ŒåŒæ—¶åº•å±‚è®¡ç®—é€šè¿‡ InfiniCore çš„é«˜æ€§èƒ½ C++ å†…æ ¸æ‰§è¡Œã€‚è¯¥å­ç³»ç»Ÿåœ¨ InfiniCore æ•´ä½“æ¶æ„ä¸­æ‰®æ¼”**æ¨ç†åº”ç”¨å±‚**çš„è§’è‰²ï¼Œè¿æ¥ç”¨æˆ·æ¨¡å‹å®šä¹‰ä¸åº•å±‚è®¡ç®—å¼•æ“ã€‚

## 2. æ¨¡å—å¯¼èˆª (Module Navigation)

### **ğŸ“‚ functional** - å‡½æ•°å¼ API å®ç°å±‚
* **åŠŸèƒ½**ï¼šæä¾›æ ¸å¿ƒç¥ç»ç½‘ç»œè®¡ç®—çš„å‡½æ•°å¼æ¥å£ï¼Œæ¯ä¸ªå‡½æ•°éƒ½æ˜¯æ— çŠ¶æ€çš„çº¯å‡½æ•°ï¼Œç›´æ¥ç»‘å®šåˆ° C++ ç®—å­å®ç°ï¼Œæ”¯æŒ in-place æ“ä½œå’Œå¯é€‰è¾“å‡ºå¼ é‡å‚æ•°ã€‚
* **èŒè´£**ï¼šå®ç°åº•å±‚ç¥ç»ç½‘ç»œåŸè¯­ï¼ŒåŒ…æ‹¬æ³¨æ„åŠ›æœºåˆ¶ï¼ˆcausal_softmaxï¼‰ã€çº¿æ€§å˜æ¢ï¼ˆlinearï¼‰ã€å½’ä¸€åŒ–ï¼ˆrms_normï¼‰ã€æ¿€æ´»å‡½æ•°ï¼ˆsilu, swigluï¼‰ã€ä½ç½®ç¼–ç ï¼ˆropeï¼‰ã€åµŒå…¥æŸ¥æ‰¾ï¼ˆembeddingï¼‰ã€éšæœºé‡‡æ ·ï¼ˆrandom_sampleï¼‰ã€‚

### **ğŸ“‚ modules** - é¢å‘å¯¹è±¡æ¨¡å—å°è£…å±‚
* **åŠŸèƒ½**ï¼šæä¾› PyTorch å…¼å®¹çš„ç¥ç»ç½‘ç»œå±‚æŠ½è±¡ï¼Œæ‰€æœ‰æ¨¡å—ç»§æ‰¿è‡ª InfiniCoreModule åŸºç±»ï¼Œå®ç°å‚æ•°æ³¨å†Œã€çŠ¶æ€å­—å…¸åºåˆ—åŒ–ã€æ¨¡å—å±‚æ¬¡ç®¡ç†å’Œå‰å‘ä¼ æ’­è®¡ç®—ã€‚
* **èŒè´£**ï¼š
  * **module.py**ï¼šæ ¸å¿ƒåŸºç±» InfiniCoreModuleï¼Œå®ç°å‚æ•°/ç¼“å†²åŒºæ³¨å†Œã€state_dict åºåˆ—åŒ–ã€æ¨¡å—å±‚æ¬¡éå†ç­‰åŸºç¡€è®¾æ–½ã€‚
  * **container.py**ï¼šModuleList å®¹å™¨ï¼Œæä¾›ç±»åˆ—è¡¨æ¥å£çš„æ¨¡å—é›†åˆç®¡ç†ã€‚
  * **linear.py**ï¼šLinear å±‚ï¼Œå®ç°ä»¿å°„å˜æ¢ y = xA^T + bã€‚
  * **normalization.py**ï¼šRMSNorm å±‚ï¼Œå®ç° RMS å±‚å½’ä¸€åŒ–ï¼ˆRoot Mean Square Layer Normalizationï¼‰ã€‚
  * **rope.py**ï¼šRoPE æ¨¡å—ï¼Œå®ç°æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRotary Position Embeddingï¼‰ï¼Œé¢„è®¡ç®— sin/cos æŸ¥æ‰¾è¡¨ã€‚
  * **sparse.py**ï¼šEmbedding å±‚ï¼Œå®ç°ç¨€ç–æŸ¥æ‰¾è¡¨æ“ä½œï¼ˆè¯åµŒå…¥ï¼‰ã€‚

### **ğŸ“„ parameter.py** - å‚æ•°ç±»å‹å®šä¹‰ï¼ˆæ ¹ç›®å½•æ–‡ä»¶ï¼‰
* **åŠŸèƒ½**ï¼šå®šä¹‰ InfiniCoreParameter ç±»å‹ï¼Œä½œä¸º InfiniCore.Tensor çš„åŒ…è£…å™¨ï¼Œç”¨äºæ¨¡å—çš„å¯è®­ç»ƒå‚æ•°æ ‡è¯†ã€‚
* **èŒè´£**ï¼šåŒºåˆ†æ™®é€šå¼ é‡ä¸å¯å­¦ä¹ å‚æ•°ï¼Œå‚ä¸æ¨¡å—çš„å‚æ•°æ³¨å†Œå’ŒçŠ¶æ€åºåˆ—åŒ–æœºåˆ¶ã€‚

## 3. æ¶æ„é€»è¾‘å›¾è§£

### 3.1 åŒå±‚æ¶æ„å…³ç³»

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç”¨æˆ·æ¨¡å‹å®šä¹‰å±‚                             â”‚
â”‚  (ç”¨æˆ·ä½¿ç”¨ modules å±‚çš„ Linear, RMSNorm, Embedding ç­‰)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   modules æ¨¡å—å°è£…å±‚                          â”‚
â”‚  InfiniCoreModule (åŸºç±»)                                      â”‚
â”‚    â”œâ”€â”€ å‚æ•°ç®¡ç† (_parameters, _buffers, _modules)              â”‚
â”‚    â”œâ”€â”€ çŠ¶æ€åºåˆ—åŒ– (state_dict, load_state_dict)               â”‚
â”‚    â””â”€â”€ å‰å‘ä¼ æ’­ (forward æ–¹æ³•)                                 â”‚
â”‚                                                               â”‚
â”‚  å…·ä½“æ¨¡å—: Linear, RMSNorm, RoPE, Embedding, ModuleList        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ è°ƒç”¨
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  functional å‡½æ•°å¼å±‚                          â”‚
â”‚  æ— çŠ¶æ€çº¯å‡½æ•°æ¥å£                                              â”‚
â”‚    â”œâ”€â”€ çº¿æ€§å˜æ¢: linear(input, weight, bias)                  â”‚
â”‚    â”œâ”€â”€ å½’ä¸€åŒ–: rms_norm(input, normalized_shape, weight)       â”‚
â”‚    â”œâ”€â”€ æ¿€æ´»å‡½æ•°: silu(input), swiglu(gate, value)            â”‚
â”‚    â”œâ”€â”€ ä½ç½®ç¼–ç : rope(x, pos_ids, sin_table, cos_table)      â”‚
â”‚    â”œâ”€â”€ æ³¨æ„åŠ›: causal_softmax(input)                          â”‚
â”‚    â”œâ”€â”€ åµŒå…¥: embedding(input, weight)                        â”‚
â”‚    â””â”€â”€ é‡‡æ ·: random_sample(logits, topp, topk, temperature)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ ç»‘å®š
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               C++ åç«¯ç®—å­å®ç° (_infinicore)                  â”‚
â”‚  é«˜æ€§èƒ½è®¡ç®—å†…æ ¸ (æ”¯æŒå¤šç¡¬ä»¶åç«¯: CPU, CUDA, MUSA ç­‰)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 æ•°æ®æµå‘

**å‰å‘ä¼ æ’­ï¼ˆForward Passï¼‰æ•°æ®æµ**ï¼š

```
è¾“å…¥æ•°æ® (Tensor)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Embedding å±‚       â”‚
â”‚    modules.Embedding â”‚  â”€â”€è°ƒç”¨â”€â”€>  functional.embedding()
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  (æŸ¥è¡¨æ“ä½œ)
    â”‚
    â–¼ è¾“å‡º: (batch, seq_len, hidden_dim)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Transformer Block  â”‚
â”‚    â”œâ”€â”€ Linear (QKV)   â”‚  â”€â”€è°ƒç”¨â”€â”€>  functional.linear()
â”‚    â”œâ”€â”€ RoPE           â”‚  â”€â”€è°ƒç”¨â”€â”€>  functional.rope()
â”‚    â”œâ”€â”€ Attention      â”‚  â”€â”€è°ƒç”¨â”€â”€>  functional.causal_softmax()
â”‚    â”œâ”€â”€ Linear (Out)   â”‚  â”€â”€è°ƒç”¨â”€â”€>  functional.linear()
â”‚    â””â”€â”€ RMSNorm        â”‚  â”€â”€è°ƒç”¨â”€â”€>  functional.rms_norm()
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼ è¾“å‡º: (batch, seq_len, hidden_dim)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. è¾“å‡ºæŠ•å½±å±‚         â”‚
â”‚    modules.Linear    â”‚  â”€â”€è°ƒç”¨â”€â”€>  functional.linear()
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼ æœ€ç»ˆè¾“å‡º: (batch, seq_len, vocab_size)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. é‡‡æ · (ç”Ÿæˆ)         â”‚
â”‚    functional        â”‚  â”€â”€è°ƒç”¨â”€â”€>  functional.random_sample()
â”‚    .random_sample    â”‚               (top-p/top-ké‡‡æ ·)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼ é‡‡æ · Token ID
```

### 3.3 æ¨¡å—ç»„åˆä¸å±‚æ¬¡ç»“æ„

```
InfiniCoreModule (æ ¹æ¨¡å—)
    â”‚
    â”œâ”€â”€ å‚æ•° (_parameters): OrderedDict[str, Parameter]
    â”‚   â”œâ”€â”€ "weight": Parameter(...)
    â”‚   â””â”€â”€ "bias": Parameter(...)
    â”‚
    â”œâ”€â”€ ç¼“å†²åŒº (_buffers): OrderedDict[str, Tensor]
    â”‚   â”œâ”€â”€ "_sin_table": Tensor(...)  # RoPE é¢„è®¡ç®—è¡¨
    â”‚   â””â”€â”€ "_cos_table": Tensor(...)
    â”‚
    â””â”€â”€ å­æ¨¡å— (_modules): OrderedDict[str, InfiniCoreModule]
        â”‚
        â”œâ”€â”€ "embedding": Embedding (InfiniCoreModule)
        â”‚       â””â”€â”€ _parameters: {"weight": Parameter}
        â”‚
        â”œâ”€â”€ "layers": ModuleList (InfiniCoreModule)
        â”‚       â””â”€â”€ _modules: {"0": TransformerBlock, "1": ...}
        â”‚               â”‚
        â”‚               â””â”€â”€ TransformerBlock (InfiniCoreModule)
        â”‚                       â”œâ”€â”€ _modules: {"attention": ..., "mlp": ...}
        â”‚                       â””â”€â”€ _parameters: {"qkv_proj.weight", ...}
        â”‚
        â””â”€â”€ "norm": RMSNorm (InfiniCoreModule)
                â””â”€â”€ _parameters: {"weight": Parameter}
```

**çŠ¶æ€åºåˆ—åŒ–ï¼ˆstate_dictï¼‰**ï¼š

```
model.state_dict() é€’å½’éå†æ¨¡å—æ ‘
    â”‚
    â”œâ”€â”€ "embedding.weight": Tensor([vocab_size, hidden_dim])
    â”œâ”€â”€ "layers.0.qkv_proj.weight": Tensor([3*hidden, hidden])
    â”œâ”€â”€ "layers.0.out_proj.weight": Tensor([hidden, hidden])
    â”œâ”€â”€ "layers.0.norm.weight": Tensor([hidden_dim])
    â”œâ”€â”€ "layers.0.rope._sin_table": Tensor([max_pos, head_dim//2])
    â”œâ”€â”€ "layers.0.rope._cos_table": Tensor([max_pos, head_dim//2])
    â”œâ”€â”€ "layers.1.qkv_proj.weight": ...
    â””â”€â”€ ...
```

### 3.4 æ¨¡å—å±‚ä¸å‡½æ•°å±‚åä½œæ¨¡å¼

**æ¨¡å¼ 1ï¼šæ¨¡å—å°è£…å‡½æ•°ï¼ˆå…¸å‹æ¨¡å¼ï¼‰**

```python
# modules å±‚ï¼šæœ‰çŠ¶æ€å°è£…
class Linear(InfiniCoreModule):
    def __init__(self, in_features, out_features, bias=False):
        super().__init__()
        # æ³¨å†Œå‚æ•°ï¼ˆç”Ÿå‘½å‘¨æœŸç”±æ¨¡å—ç®¡ç†ï¼‰
        self.weight = Parameter(infinicore.empty([out_features, in_features]))
        if bias:
            self.bias = Parameter(infinicore.empty([out_features]))

    def forward(self, input: Tensor) -> Tensor:
        # è°ƒç”¨ functional å±‚çš„æ— çŠ¶æ€å‡½æ•°
        return F.linear(input, self.weight, self.bias)

# functional å±‚ï¼šæ— çŠ¶æ€å‡½æ•°
def linear(input, weight, bias=None, *, out=None):
    # ç›´æ¥è°ƒç”¨ C++ ç»‘å®š
    return _infinicore.linear(input._underlying, weight._underlying, ...)
```

**æ¨¡å¼ 2ï¼šæ¨¡å—é¢„è®¡ç®— + å‡½æ•°åº”ç”¨ï¼ˆRoPE æ¡ˆä¾‹ï¼‰**

```python
# modules å±‚ï¼šåˆå§‹åŒ–æ—¶é¢„è®¡ç®—æŸ¥æ‰¾è¡¨
class RoPE(InfiniCoreModule):
    def __init__(self, max_position_embeddings, rope_theta, head_dim):
        super().__init__()
        # é¢„è®¡ç®— sin/cos è¡¨ï¼ˆä¸€æ¬¡æ€§è®¡ç®—ï¼Œå­˜å‚¨ä¸ºç¼“å†²åŒºï¼‰
        sin_table, cos_table = self.create_sin_cos_table(...)
        self.register_buffer("_sin_table", sin_table)
        self.register_buffer("_cos_table", cos_table)

    def forward(self, states, position_ids, algo):
        # ä½¿ç”¨é¢„è®¡ç®—è¡¨è°ƒç”¨ functional å‡½æ•°
        return F.rope(states, position_ids,
                      self._sin_table, self._cos_table, algo, out=states)

# functional å±‚ï¼šä½¿ç”¨æŸ¥æ‰¾è¡¨åº”ç”¨ä½ç½®ç¼–ç 
def rope(x, pos_ids, sin_table, cos_table, algo, *, out=None):
    return _infinicore.rope(x._underlying, pos_ids._underlying, ...)
```

**æ¨¡å¼ 3ï¼šå®¹å™¨ç®¡ç†æ¨¡å—åˆ—è¡¨ï¼ˆModuleList æ¡ˆä¾‹ï¼‰**

```python
# ç”¨æˆ·å®šä¹‰å¤šå±‚ç½‘ç»œ
class Transformer(InfiniCoreModule):
    def __init__(self, num_layers, hidden_dim):
        super().__init__()
        # ä½¿ç”¨ ModuleList ç®¡ç†å¤šä¸ªå­æ¨¡å—
        self.layers = ModuleList([
            TransformerBlock(hidden_dim) for _ in range(num_layers)
        ])

    def forward(self, x):
        # éå† ModuleList é€å±‚è®¡ç®—
        for layer in self.layers:
            x = layer(x)
        return x

# ModuleList å†…éƒ¨ä½¿ç”¨ OrderedDict å­˜å‚¨æ¨¡å—
# _modules = {"0": TransformerBlock, "1": TransformerBlock, ...}
```

### 3.5 ç¡¬ä»¶åŠ é€Ÿè·¯å¾„

```
ç”¨æˆ·è°ƒç”¨ silu(input)
    â”‚
    â–¼
functional.silu(input, inplace=False, out=None)
    â”‚
    â”œâ”€â”€ æ£€æŸ¥åŠ é€Ÿæ¡ä»¶
    â”‚   â”œâ”€â”€ infinicore.use_ntops == True?
    â”‚   â”œâ”€â”€ device in ["cuda", "musa"]?
    â”‚   â””â”€â”€ out is None?
    â”‚
    â”œâ”€â”€ æ»¡è¶³æ¡ä»¶ â†’ ä½¿ç”¨ ntops.torch.silu() (ç¡¬ä»¶ä¼˜åŒ–è·¯å¾„)
    â”‚
    â””â”€â”€ ä¸æ»¡è¶³ â†’ ä½¿ç”¨ _infinicore.silu() (é€šç”¨ C++ è·¯å¾„)
                â”‚
                â”œâ”€â”€ inplace == True â†’ _infinicore.silu_() (åŸåœ°ä¿®æ”¹)
                â””â”€â”€ inplace == False â†’ _infinicore.silu() (æ–°å¼ é‡)
```

### 3.6 å†…å­˜ä¼˜åŒ–ç­–ç•¥

**In-Place æ“ä½œä¼˜åŒ–é“¾**ï¼š

```python
# å†…å­˜ä¼˜åŒ–çš„ FFN å‰å‘ä¼ æ’­
def memory_efficient_ffn(x, w_gate, w_up, w_down, norm_weight):
    # 1. çº¿æ€§å˜æ¢ï¼ˆå¿…é¡»åˆ›å»ºæ–°å¼ é‡ï¼‰
    gate = F.linear(x, w_gate)  # æ–°å¼ é‡
    up = F.linear(x, w_up)      # æ–°å¼ é‡

    # 2. é‡ç”¨ gate å¼ é‡è¿›è¡Œ in-place SiLU
    F.silu(gate, inplace=True)  # åŸåœ°ä¿®æ”¹ï¼Œæ— æ–°åˆ†é…

    # 3. SwiGLU ç»“æœå†™å…¥ gate å¼ é‡ï¼Œé‡ç”¨å†…å­˜
    F.swiglu(gate, up, out=gate)  # gate è¢«è¦†ç›–

    # 4. è¾“å‡ºæŠ•å½±
    output = F.linear(gate, w_down)  # æ–°å¼ é‡

    # 5. RMS å½’ä¸€åŒ– in-place
    F.rms_norm(output, [output.shape[-1]], norm_weight, out=output)

    return output  # ä»…åˆ†é… 3 ä¸ªå¼ é‡ï¼ˆgate, up, outputï¼‰ï¼Œè€Œé 6 ä¸ª
```

**å‚æ•°å¤ç”¨ä¸å…±äº«**ï¼š

```python
# æƒé‡å…±äº«ï¼ˆå¤šä¸ªå±‚ä½¿ç”¨åŒä¸€å‚æ•°ï¼‰
class SharedWeightModel(InfiniCoreModule):
    def __init__(self, hidden_dim):
        super().__init__()
        self.weight = Parameter(...)

        # å¤šä¸ªæ¨¡å—å…±äº«åŒä¸€å‚æ•°ï¼ˆå¼•ç”¨åŒä¸€å¯¹è±¡ï¼‰
        self.layer1 = Linear(hidden_dim, hidden_dim)
        self.layer1.weight = self.weight  # å…±äº«

        self.layer2 = Linear(hidden_dim, hidden_dim)
        self.layer2.weight = self.weight  # å…±äº«

    # state_dict ä»…ä¿å­˜ä¸€ä»½æƒé‡
    # "layer1.weight" å’Œ "layer2.weight" æŒ‡å‘åŒä¸€å¯¹è±¡
```

## 4. è®¾è®¡åŸåˆ™ä¸æœ€ä½³å®è·µ

### 4.1 èŒè´£åˆ†ç¦»

* **functional å±‚**ï¼šä¸“æ³¨äºè®¡ç®—é€»è¾‘ï¼Œä¿æŒæ— çŠ¶æ€ã€å¯ç»„åˆã€å¯æµ‹è¯•ã€‚ä¸ç®¡ç†å‚æ•°ç”Ÿå‘½å‘¨æœŸï¼Œä¸ç»´æŠ¤å†…éƒ¨çŠ¶æ€ã€‚
* **modules å±‚**ï¼šä¸“æ³¨äºçŠ¶æ€ç®¡ç†ï¼Œè´Ÿè´£å‚æ•°æ³¨å†Œã€æ¨¡å—ç»„åˆã€åºåˆ—åŒ–ã€å‰å‘ä¼ æ’­ç¼–æ’ã€‚ä¸ç›´æ¥å®ç°è®¡ç®—é€»è¾‘ï¼Œå§”æ‰˜ç»™ functional å±‚ã€‚

### 4.2 PyTorch å…¼å®¹æ€§

* **API ä¸€è‡´æ€§**ï¼šå‡½æ•°ç­¾åã€å‚æ•°å‘½åã€è¿”å›å€¼ç±»å‹ä¸ PyTorch å¯¹é½ï¼ˆå¦‚ `Linear(in_features, out_features, bias=False)`ï¼‰ã€‚
* **çŠ¶æ€å­—å…¸æ ¼å¼**ï¼šä½¿ç”¨ç‚¹åˆ†éš”çš„å±‚æ¬¡åŒ–é”®åï¼ˆå¦‚ `layers.0.weight`ï¼‰ï¼Œä¸ PyTorch æ¨¡å‹äº’æ“ä½œã€‚
* **æ¨¡å—ç»„åˆæ¨¡å¼**ï¼šæ”¯æŒåµŒå¥—å­æ¨¡å—ã€å‚æ•°å…±äº«ã€ModuleList å®¹å™¨ç­‰ PyTorch æƒ¯ç”¨æ³•ã€‚

### 4.3 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

* **é¢„è®¡ç®—ç­–ç•¥**ï¼šRoPE åœ¨åˆå§‹åŒ–æ—¶é¢„è®¡ç®— sin/cos æŸ¥æ‰¾è¡¨ï¼Œé¿å…å‰å‘ä¼ æ’­é‡å¤è®¡ç®—ã€‚
* **In-Place æ“ä½œ**ï¼šæä¾› `inplace=True` å’Œ `out=` å‚æ•°æ”¯æŒå†…å­˜é‡ç”¨ï¼Œå‡å°‘å¤§æ¨¡å‹æ¨ç†çš„å†…å­˜å ç”¨ã€‚
* **ç¡¬ä»¶åŠ é€Ÿ**ï¼šé€šè¿‡ `infinicore.use_ntops` é…ç½®ï¼Œé€‰æ‹©ç¡¬ä»¶ä¼˜åŒ–ç®—å­åº“ï¼ˆå¦‚ NVIDIA/MUSA GPU çš„ ntopsï¼‰ã€‚
* **ç®—å­èåˆ**ï¼šC++ å±‚å¯èƒ½èåˆå¤šä¸ªæ“ä½œï¼ˆå¦‚ softmax + å› æœæ©ç èåˆä¸º causal_softmaxï¼‰ã€‚

### 4.4 æ‰©å±•æ€§æŒ‡å—

**æ·»åŠ æ–°å‡½æ•°ï¼ˆfunctional å±‚ï¼‰**ï¼š

1. åœ¨ C++ å±‚å®ç°ç®—å­ï¼ˆæ·»åŠ åˆ° `_infinicore` æ‰©å±•æ¨¡å—ï¼‰ã€‚
2. åœ¨ `functional/` ç›®å½•åˆ›å»ºå¯¹åº” Python æ–‡ä»¶ï¼Œç¼–å†™åŒ…è£…å‡½æ•°ã€‚
3. éµå¾ªå‘½åçº¦å®šï¼šé in-place ç‰ˆæœ¬è°ƒç”¨ `function()`ï¼Œin-place ç‰ˆæœ¬è°ƒç”¨ `function_()`ã€‚
4. æ”¯æŒå¯é€‰ `out` å‚æ•°ç”¨äºå†…å­˜ä¼˜åŒ–ã€‚
5. åœ¨ `functional/__init__.py` ä¸­å¯¼å‡ºå‡½æ•°ã€‚

**æ·»åŠ æ–°æ¨¡å—ï¼ˆmodules å±‚ï¼‰**ï¼š

1. ç»§æ‰¿ `InfiniCoreModule` åŸºç±»ã€‚
2. åœ¨ `__init__` ä¸­é€šè¿‡ `self.param_name = Parameter(...)` æ³¨å†Œå‚æ•°ã€‚
3. é€šè¿‡ `register_buffer()` æ³¨å†Œéå‚æ•°å¼ é‡ï¼ˆå¦‚é¢„è®¡ç®—è¡¨ã€è¿è¡Œç»Ÿè®¡ï¼‰ã€‚
4. å®ç° `forward()` æ–¹æ³•ï¼Œè°ƒç”¨ `functional` å±‚çš„å‡½æ•°å®Œæˆè®¡ç®—ã€‚
5. å®ç° `extra_repr()` è¿”å›æ¨¡å—å…³é”®é…ç½®ä¿¡æ¯ï¼ˆå¦‚ `in_features`, `out_features`ï¼‰ã€‚
6. åœ¨ `modules/__init__.py` ä¸­å¯¼å‡ºæ–°æ¨¡å—ã€‚

## 5. å…¸å‹åº”ç”¨åœºæ™¯

### åœºæ™¯ 1ï¼šæ„å»º Transformer è¯­è¨€æ¨¡å‹

```python
class LlamaLikeModel(InfiniCoreModule):
    def __init__(self, vocab_size, hidden_dim, num_layers, num_heads):
        super().__init__()
        self.embedding = Embedding(vocab_size, hidden_dim)
        self.layers = ModuleList([
            TransformerBlock(hidden_dim, num_heads)
            for _ in range(num_layers)
        ])
        self.norm = RMSNorm(hidden_dim)

    def forward(self, input_ids, position_ids):
        # 1. è¯åµŒå…¥
        hidden = self.embedding(input_ids)

        # 2. å †å  Transformer å±‚
        for layer in self.layers:
            hidden = layer(hidden, position_ids)

        # 3. æœ€ç»ˆå½’ä¸€åŒ–
        hidden = self.norm(hidden)

        # 4. æŠ•å½±åˆ°è¯è¡¨
        logits = linear(hidden, self.embedding.weight.t())  # æƒé‡å…±äº«

        return logits

# ä¿å­˜/åŠ è½½æ¨¡å‹æƒé‡
state_dict = model.state_dict()  # ä¿å­˜
model.load_state_dict(state_dict)  # åŠ è½½
```

### åœºæ™¯ 2ï¼šå¤§è¯­è¨€æ¨¡å‹æ¨ç†ï¼ˆå«é‡‡æ ·ï¼‰

```python
def generate_text(model, prompt_ids, max_tokens=100):
    """è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆ"""
    generated = prompt_ids.tolist()

    for _ in range(max_tokens):
        input_ids = Tensor(generated)
        position_ids = Tensor.arange(len(generated)).unsqueeze(0)

        # 1. å‰å‘ä¼ æ’­è·å– logits
        logits = model(input_ids, position_ids)  # [1, seq_len, vocab_size]

        # 2. å–æœ€åä¸€ä¸ªä½ç½®çš„ logits
        next_token_logits = logits[0, -1, :]  # [vocab_size]

        # 3. nucleus/top-k é‡‡æ ·
        random_val = random.random()
        next_token = random_sample(
            logits=next_token_logits,
            random_val=random_val,
            topp=0.9,
            topk=50,
            temperature=0.8
        )

        # 4. æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
        generated.append(next_token.item())

        # 5. æ£€æŸ¥ç»“æŸç¬¦
        if next_token.item() == eos_token_id:
            break

    return generated
```

### åœºæ™¯ 3ï¼šå†…å­˜ä¼˜åŒ–çš„æ‰¹é‡æ¨ç†

```python
def batch_inference_efficient(model, input_ids_batch, position_ids_batch):
    """ä½¿ç”¨ in-place æ“ä½œä¼˜åŒ–æ‰¹é‡æ¨ç†å†…å­˜"""
    batch_outputs = []

    for input_ids, position_ids in zip(input_ids_batch, position_ids_batch):
        # æ¿€æ´» in-place æ¨¡å¼å‡å°‘å†…å­˜åˆ†é…
        output = model(input_ids, position_ids)

        # å¯¹è¾“å‡ºè¿›è¡Œ in-place å½’ä¸€åŒ–
        rms_norm(output, [output.shape[-1]], model.norm.weight, out=output)

        batch_outputs.append(output)

    return batch_outputs
```

## 6. ä¾èµ–å…³ç³»å›¾

```
infinicore.nn
    â”‚
    â”œâ”€â”€ å†…éƒ¨ä¾èµ–
    â”‚   â”œâ”€â”€ infinicore.Tensor (å¼ é‡ç±»å‹)
    â”‚   â”œâ”€â”€ infinicore.Parameter (å‚æ•°ç±»å‹)
    â”‚   â”œâ”€â”€ infinicore.device (è®¾å¤‡ç®¡ç†)
    â”‚   â”œâ”€â”€ infinicore.empty, from_numpy (å¼ é‡æ„é€ )
    â”‚   â””â”€â”€ infinicore.lib._infinicore (C++ æ‰©å±•æ¨¡å—)
    â”‚
    â”œâ”€â”€ Python æ ‡å‡†åº“
    â”‚   â”œâ”€â”€ collections.OrderedDict
    â”‚   â”œâ”€â”€ typing (ç±»å‹æ³¨è§£)
    â”‚   â”œâ”€â”€ itertools.chain
    â”‚   â””â”€â”€ numbers.Integral
    â”‚
    â””â”€â”€ å¤–éƒ¨ä¾èµ–ï¼ˆæ¡ä»¶ä¾èµ–ï¼‰
        â”œâ”€â”€ numpy (RoPE é¢„è®¡ç®—ï¼Œå¯æ›¿æ¢)
        â””â”€â”€ ntops (ç¡¬ä»¶åŠ é€Ÿåº“ï¼Œå¯é€‰)
```

## 7. æ€§èƒ½ç‰¹å¾

* **è®¡ç®—å¤æ‚åº¦**ï¼š
  * Linear: O(batch_size * in_features * out_features)
  * causal_softmax: O(batch_size * num_heads * seq_len^2) - æ³¨æ„åŠ›ç“¶é¢ˆ
  * rms_norm: O(batch_size * seq_len * hidden_dim)
  * rope: O(batch_size * seq_len * num_heads * head_dim) - æŸ¥æ‰¾è¡¨æ“ä½œ

* **å†…å­˜å ç”¨**ï¼š
  * æ¨¡å—å‚æ•°ï¼šO(total_parameters) - ç”±æ¨¡å‹å¤§å°å†³å®š
  * RoPE æŸ¥æ‰¾è¡¨ï¼šO(max_position_embeddings * head_dim) - å›ºå®šå¼€é”€
  * å‰å‘ä¼ æ’­ä¸­é—´ç»“æœï¼šO(batch_size * seq_len * hidden_dim * num_layers)

* **ä¼˜åŒ–çº§åˆ«**ï¼š
  * C++ å†…æ ¸ï¼šä½¿ç”¨ SIMDã€å¹¶è¡Œç®—æ³•ã€ç®—å­èåˆ
  * ç¡¬ä»¶åŠ é€Ÿï¼šé’ˆå¯¹ CUDA/MUSA çš„ä¼˜åŒ–å†…æ ¸ï¼ˆntopsï¼‰
  * Python å±‚ï¼šæœ€å°åŒ–å¼€é”€ï¼Œç›´æ¥è½¬å‘åˆ° C++ å±‚
