# Python InfiniCore åŒ…æ¶æ„å…¨æ™¯

## 1. å­ç³»ç»ŸèŒè´£

æœ¬ç›®å½• `python` æ˜¯ InfiniCore æ·±åº¦å­¦ä¹ æ¨ç†æ¡†æ¶çš„ **Python å‰ç«¯åŒ…æ ¹ç›®å½•**ï¼Œæä¾›äº†ä» Python API åˆ°åº•å±‚ C++/CUDA å®ç°çš„å®Œæ•´ç»‘å®šå±‚ã€‚è¯¥å­ç³»ç»Ÿé‡‡ç”¨**åˆ†å±‚æ¶æ„è®¾è®¡**ï¼Œå®ç°äº†ä»é«˜å±‚ç¥ç»ç½‘ç»œæŠ½è±¡åˆ°ä½å±‚å¼ é‡è®¡ç®—çš„å…¨æ ˆè¦†ç›–ï¼š

* **ç¥ç»ç½‘ç»œå±‚ï¼ˆnnï¼‰**ï¼šæä¾›ä¸ PyTorch å…¼å®¹çš„é«˜å±‚ç¥ç»ç½‘ç»œæ¨¡å—ï¼ŒåŒ…æ‹¬æœ‰çŠ¶æ€çš„ç»„ä»¶å°è£…ï¼ˆmodulesï¼‰å’Œæ— çŠ¶æ€çš„å‡½æ•°å¼æ¥å£ï¼ˆfunctionalï¼‰ï¼Œæ„å»ºç”¨æˆ·å¯ç›´æ¥ä½¿ç”¨çš„æ¨ç†æ¨¡å‹ã€‚
* **ç®—å­å±‚ï¼ˆopsï¼‰**ï¼šæä¾›åº•å±‚è®¡ç®—æ“ä½œçš„ Python ç»‘å®šï¼ŒåŒ…æ‹¬åŸºç¡€ç®—æœ¯è¿ç®—ã€å¼ é‡å½¢çŠ¶æ“ä½œã€æ ‡å‡†æ³¨æ„åŠ›å’Œåˆ†é¡µæ³¨æ„åŠ›ï¼ˆPaged Attentionï¼‰ç­‰é«˜æ€§èƒ½ç®—å­ã€‚
* **æ ¸å¿ƒç±»å‹å±‚ï¼ˆæ ¹ç›®å½•ï¼‰**ï¼šæä¾›æ¡†æ¶çš„æ ¸å¿ƒæ•°æ®ç»“æ„å’Œç®¡ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬å¼ é‡ï¼ˆTensorï¼‰ã€è®¾å¤‡ï¼ˆdeviceï¼‰ã€æ•°æ®ç±»å‹ï¼ˆdtypeï¼‰ã€ä¸Šä¸‹æ–‡ç®¡ç†ï¼ˆcontextï¼‰å’Œäº‹ä»¶åŒæ­¥ï¼ˆdevice_eventï¼‰ç­‰åŸºç¡€è®¾æ–½ã€‚

è¿™ç§è®¾è®¡ä½¿ç”¨æˆ·èƒ½å¤Ÿä½¿ç”¨ç†Ÿæ‚‰çš„ PyTorch é£æ ¼ API å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ŒåŒæ—¶åº•å±‚è®¡ç®—é€šè¿‡ InfiniCore çš„é«˜æ€§èƒ½ C++ å†…æ ¸æ‰§è¡Œï¼Œæ”¯æŒå¤šç§ç¡¬ä»¶åç«¯ï¼ˆCPUã€CUDAã€MUSA ç­‰ï¼‰ã€‚è¯¥å­ç³»ç»Ÿåœ¨ InfiniCore æ•´ä½“æ¶æ„ä¸­æ‰®æ¼”**Python ç”¨æˆ·ç•Œé¢å±‚**çš„è§’è‰²ï¼Œæ˜¯æ¡†æ¶ä¸ç”¨æˆ·äº¤äº’çš„æ¡¥æ¢ã€‚

## 2. æ¨¡å—å¯¼èˆª (Module Navigation)

### **ğŸ“‚ infinicore** - Python å‰ç«¯æ ¸å¿ƒåŒ…
* **åŠŸèƒ½**ï¼šæä¾›å®Œæ•´çš„ Python ç»‘å®šå±‚ï¼Œå®ç°ä» Python API åˆ° C++ åç«¯çš„æ— ç¼å¯¹æ¥ï¼Œé‡‡ç”¨åˆ†å±‚æ¶æ„è®¾è®¡ï¼ˆç¥ç»ç½‘ç»œå±‚ â†’ ç®—å­å±‚ â†’ æ ¸å¿ƒç±»å‹å±‚ï¼‰ã€‚
* **èŒè´£**ï¼š
  * **nn å­æ¨¡å—**ï¼šPyTorch å…¼å®¹çš„ç¥ç»ç½‘ç»œå±‚æŠ½è±¡ï¼Œé‡‡ç”¨åŒå±‚æ¶æ„ï¼ˆfunctional + modulesï¼‰å®ç°å¯å¤ç”¨çš„ç¥ç»ç½‘ç»œç»„ä»¶ã€‚
  * **ops å­æ¨¡å—**ï¼šåº•å±‚ç®—å­ç»‘å®šå±‚ï¼Œæä¾›ç®—æœ¯è¿ç®—ã€èåˆç®—å­ã€å½¢çŠ¶æ“ä½œå’Œæ³¨æ„åŠ›ç®—å­çš„ Python æ¥å£ã€‚
  * **æ ¸å¿ƒç±»å‹ç³»ç»Ÿ**ï¼šTensorï¼ˆå¼ é‡å°è£…ï¼‰ã€deviceï¼ˆè®¾å¤‡æŠ½è±¡ï¼‰ã€dtypeï¼ˆæ•°æ®ç±»å‹ï¼‰ã€contextï¼ˆä¸Šä¸‹æ–‡ç®¡ç†ï¼‰ã€device_eventï¼ˆäº‹ä»¶åŒæ­¥ï¼‰ã€graphï¼ˆè®¡ç®—å›¾ï¼‰ç­‰åŸºç¡€è®¾æ–½ã€‚
  * **å…¬å…± API å¯¼å‡º**ï¼šé€šè¿‡ `__init__.py` ç»Ÿä¸€å¯¼å‡ºæ‰€æœ‰æ ¸å¿ƒç±»ã€å‡½æ•°å’Œå­æ¨¡å—ï¼Œé…ç½®ç¡¬ä»¶åŠ é€Ÿåº“ï¼ˆntopsï¼‰ã€‚

## 3. æ¶æ„é€»è¾‘å›¾è§£

### 3.1 åˆ†å±‚æ¶æ„å…³ç³»

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ç”¨æˆ·åº”ç”¨å±‚                                â”‚
â”‚  (ç”¨æˆ·ä½¿ç”¨ nn.modules çš„ Linear, RMSNorm, Embedding ç­‰å®šä¹‰æ¨¡å‹) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   nn.modules æ¨¡å—å°è£…å±‚                       â”‚
â”‚  InfiniCoreModule (åŸºç±»)                                      â”‚
â”‚    â”œâ”€â”€ å‚æ•°ç®¡ç† (_parameters, _buffers, _modules)              â”‚
â”‚    â”œâ”€â”€ çŠ¶æ€åºåˆ—åŒ– (state_dict, load_state_dict)               â”‚
â”‚    â””â”€â”€ å‰å‘ä¼ æ’­ (forward æ–¹æ³•)                                 â”‚
â”‚                                                               â”‚
â”‚  å…·ä½“æ¨¡å—: Linear, RMSNorm, RoPE, Embedding, ModuleList        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ è°ƒç”¨
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  nn.functional å‡½æ•°å¼å±‚                       â”‚
â”‚  æ— çŠ¶æ€çº¯å‡½æ•°æ¥å£                                              â”‚
â”‚    â”œâ”€â”€ çº¿æ€§å˜æ¢: linear(input, weight, bias)                  â”‚
â”‚    â”œâ”€â”€ å½’ä¸€åŒ–: rms_norm(input, normalized_shape, weight)       â”‚
â”‚    â”œâ”€â”€ æ¿€æ´»å‡½æ•°: silu(input), swiglu(gate, value)            â”‚
â”‚    â”œâ”€â”€ ä½ç½®ç¼–ç : rope(x, pos_ids, sin_table, cos_table)      â”‚
â”‚    â”œâ”€â”€ æ³¨æ„åŠ›: causal_softmax(input)                          â”‚
â”‚    â”œâ”€â”€ åµŒå…¥: embedding(input, weight)                        â”‚
â”‚    â””â”€â”€ é‡‡æ ·: random_sample(logits, topp, topk, temperature)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ è°ƒç”¨ / ç»‘å®š
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ops ç®—å­ç»‘å®šå±‚                            â”‚
â”‚  Python-C++ ç»‘å®šæ¥å£                                          â”‚
â”‚    â”œâ”€â”€ åŸºç¡€ç®—æœ¯: add, mul, matmul                            â”‚
â”‚    â”œâ”€â”€ èåˆç®—å­: add_rms_norm                                â”‚
â”‚    â”œâ”€â”€ å½¢çŠ¶æ“ä½œ: narrow, squeeze, unsqueeze, rearrange      â”‚
â”‚    â”œâ”€â”€ æ³¨æ„åŠ›: attention                                     â”‚
â”‚    â””â”€â”€ åˆ†é¡µæ³¨æ„åŠ›: paged_attention, paged_attention_prefill,  â”‚
â”‚                  paged_caching                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ ç»‘å®š
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Tensor/device/dtype æ ¸å¿ƒç±»å‹å±‚                    â”‚
â”‚  Tensor ç±» (å¼ é‡å°è£…)                                         â”‚
â”‚    â”œâ”€â”€ åº•å±‚ C++ å¯¹è±¡è®¿é—® (_underlying)                        â”‚
â”‚    â”œâ”€â”€ æƒ°æ€§å±æ€§ (shape, dtype, device)                        â”‚
â”‚    â”œâ”€â”€ è§†å›¾æ“ä½œ (view, permute, squeeze)                      â”‚
â”‚    â””â”€â”€ äº’æ“ä½œ (from_numpy, from_torch, to)                    â”‚
â”‚                                                               â”‚
â”‚  device ç±» (è®¾å¤‡æŠ½è±¡)                                         â”‚
â”‚    â”œâ”€â”€ è®¾å¤‡ç±»å‹å’Œç´¢å¼• (type, index)                          â”‚
â”‚    â””â”€â”€ C++ è®¾å¤‡å¯¹è±¡è½¬æ¢                                       â”‚
â”‚                                                               â”‚
â”‚  dtype ç±» (æ•°æ®ç±»å‹)                                          â”‚
â”‚    â”œâ”€â”€ ç±»å‹å¸¸é‡ (float32, int64, etc.)                       â”‚
â”‚    â””â”€â”€ ç±»å‹è½¬æ¢æ˜ å°„                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ ç»‘å®š
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               C++ åç«¯å®ç° (_infinicore)                       â”‚
â”‚  é«˜æ€§èƒ½è®¡ç®—å†…æ ¸ (æ”¯æŒå¤šç¡¬ä»¶åç«¯: CPU, CUDA, MUSA ç­‰)            â”‚
â”‚  åˆ†é¡µæ³¨æ„åŠ›ç®—æ³•ã€ç®—å­èåˆã€å†…å­˜ç®¡ç†ã€å›¾ä¼˜åŒ–                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 æ•°æ®æµå‘

**å…¸å‹ LLM æ¨ç†æµç¨‹**ï¼š

```
è¾“å…¥ Token IDs (Tensor)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. åµŒå…¥å±‚ (nn.modules.Embedding)      â”‚
â”‚    â†“ è°ƒç”¨ functional.embedding()      â”‚
â”‚    â†“ æŸ¥è¡¨æ“ä½œå¾—åˆ°åµŒå…¥å‘é‡              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ è¾“å‡º: (batch, seq_len, hidden_dim)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Transformer Block (å¤šå±‚å †å )       â”‚
â”‚    â”œâ”€â”€ QKV æŠ•å½± (nn.modules.Linear)   â”‚
â”‚    â”‚   â†“ è°ƒç”¨ functional.linear()     â”‚
â”‚    â”‚   â†“ ops.matmul() (çŸ©é˜µä¹˜æ³•)      â”‚
â”‚    â”‚                                 â”‚
â”‚    â”œâ”€â”€ RoPE ä½ç½®ç¼–ç  (nn.modules.RoPE)â”‚
â”‚    â”‚   â†“ è°ƒç”¨ functional.rope()       â”‚
â”‚    â”‚   â†“ ä½¿ç”¨é¢„è®¡ç®—çš„ sin/cos è¡¨      â”‚
â”‚    â”‚                                 â”‚
â”‚    â”œâ”€â”€ è‡ªæ³¨æ„åŠ›è®¡ç®—                   â”‚
â”‚    â”‚   â†“ causal_softmax()            â”‚
â”‚    â”‚   â†“ æˆ–ä½¿ç”¨ ops.attention()       â”‚
â”‚    â”‚   â†“ æˆ–ä½¿ç”¨ ops.paged_attention() â”‚
â”‚    â”‚      (åˆ†é¡µæ³¨æ„åŠ›ï¼Œé«˜æ•ˆ KV cache) â”‚
â”‚    â”‚                                 â”‚
â”‚    â”œâ”€â”€ FFN (SwiGLU æ¿€æ´»)              â”‚
â”‚    â”‚   â†“ functional.swiglu()         â”‚
â”‚    â”‚   â†“ å†…éƒ¨è°ƒç”¨ silu() + é€å…ƒç´ ä¹˜   â”‚
â”‚    â”‚                                 â”‚
â”‚    â””â”€â”€ RMS å½’ä¸€åŒ– (nn.modules.RMSNorm)â”‚
â”‚        â†“ è°ƒç”¨ functional.rms_norm()   â”‚
â”‚        â†“ æˆ–ä½¿ç”¨ ops.add_rms_norm()    â”‚
â”‚          (èåˆç®—å­ï¼Œæ®‹å·®è¿æ¥ä¼˜åŒ–)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ è¾“å‡º: (batch, seq_len, hidden_dim)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. è¾“å‡ºæŠ•å½± (nn.modules.Linear)       â”‚
â”‚    â†“ æŠ•å½±åˆ°è¯è¡¨ç©ºé—´                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ è¾“å‡º: (batch, seq_len, vocab_size)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. é‡‡æ · (ç”Ÿæˆ)                        â”‚
â”‚    â†“ functional.random_sample()      â”‚
â”‚    â†“ top-p/top-k é‡‡æ ·                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ é‡‡æ · Token ID
    â–¼
è¾“å‡ºä¸‹ä¸€ä¸ª Token
```

### 3.3 åˆ†é¡µæ³¨æ„åŠ›ï¼ˆPaged Attentionï¼‰ä¼˜åŒ–æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              é˜¶æ®µ 1: é¢„å¡«å……ï¼ˆPrefillï¼‰                        â”‚
â”‚  å¤„ç†åˆå§‹ Prompt çš„æ‰€æœ‰ Token                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€ è¾“å…¥: Prompts Q (å½¢çŠ¶: [total_tokens, num_heads, head_dim])
    â”‚
    â”œâ”€â”€ è°ƒç”¨: ops.paged_attention_prefill(
    â”‚         q, k_cache, v_cache,
    â”‚         block_tables,      # å—è¡¨: é€»è¾‘é¡µ â†’ ç‰©ç†é¡µæ˜ å°„
    â”‚         history_lens,      # æ¯ä¸ªåºåˆ—çš„å†å²é•¿åº¦
    â”‚         cu_seqlens_q,      # ç´¯ç§¯åºåˆ—é•¿åº¦ï¼ˆCUDA å†…æ ¸ä½¿ç”¨ï¼‰
    â”‚         scale              # æ³¨æ„åŠ›ç¼©æ”¾å› å­
    â”‚       )
    â”‚
    â””â”€â”€ è¾“å‡º: é¢„å¡«å……é˜¶æ®µçš„æ³¨æ„åŠ›è¾“å‡º

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              é˜¶æ®µ 2: è§£ç ï¼ˆDecodeï¼‰å¾ªç¯                       â”‚
â”‚  è‡ªå›å½’ç”Ÿæˆï¼Œæ¯æ­¥ç”Ÿæˆä¸€ä¸ªæ–° Token                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚ å¾ªç¯ (æ¯ä¸ªç”Ÿæˆæ­¥éª¤):
    â”‚
    â”œâ”€â”€ 2.1 è®¡ç®— new token çš„ QKV
    â”‚     â”‚
    â”‚     â””â”€â”€ Q, K, V = model.forward(last_token)
    â”‚
    â”œâ”€â”€ 2.2 å†™å…¥ KV Cache (åˆ†é¡µç¼“å­˜)
    â”‚     â”‚
    â”‚     â”œâ”€â”€ slot_mapping = compute_slot_mapping(...)  # è®¡ç®—ç‰©ç†ä½ç½®
    â”‚     â”‚
    â”‚     â””â”€â”€ ops.paged_caching(
    â”‚            k_cache, v_cache,  # ç¼“å­˜å¼ é‡ï¼ˆå°±åœ°ä¿®æ”¹ï¼‰
    â”‚            k, v,              # æ–°çš„é”®å€¼å¯¹
    â”‚            slot_mapping       # æ§½ä½æ˜ å°„
    â”‚          )
    â”‚
    â”œâ”€â”€ 2.3 è®¡ç®—æ³¨æ„åŠ›ï¼ˆä¸å†å² Cacheï¼‰
    â”‚     â”‚
    â”‚     â””â”€â”€ ops.paged_attention(
    â”‚            q,                # new token çš„æŸ¥è¯¢
    â”‚            k_cache, v_cache, # å®Œæ•´çš„ KV cache
    â”‚            block_tables,     # å—è¡¨
    â”‚            cache_lens,       # æ¯ä¸ªåºåˆ—çš„ç¼“å­˜é•¿åº¦
    â”‚            scale             # ç¼©æ”¾å› å­
    â”‚          )
    â”‚
    â””â”€â”€ 2.4 è¾“å‡º â†’ ä¸‹ä¸€ä¸ª token
```

### 3.4 ç¡¬ä»¶åŠ é€Ÿè·¯å¾„

```
ç”¨æˆ·è°ƒç”¨ silu(input) æˆ– SwiGLU æ¿€æ´»
    â”‚
    â–¼
æ£€æŸ¥å…¨å±€é…ç½® infinicore.use_ntops
    â”‚
    â”œâ”€â”€ use_ntops == True
    â”‚   â”œâ”€â”€ æ£€æŸ¥è®¾å¤‡ç±»å‹ (device.type in ["cuda", "musa"]?)
    â”‚   â”œâ”€â”€ æ£€æŸ¥æ˜¯å¦æŒ‡å®š out å‚æ•°
    â”‚   â”‚
    â”‚   â”œâ”€â”€ æ»¡è¶³æ¡ä»¶ â†’ ä½¿ç”¨ ntops.torch.silu() (ç¡¬ä»¶ä¼˜åŒ–è·¯å¾„)
    â”‚   â”‚   â”œâ”€â”€ ntops æ˜¯é’ˆå¯¹ NVIDIA/MUSA GPU çš„ä¼˜åŒ–ç®—å­åº“
    â”‚   â”‚   â””â”€â”€ æä¾›æ¯”çº¯ C++ å®ç°æ›´é«˜çš„æ€§èƒ½
    â”‚   â”‚
    â”‚   â””â”€â”€ ä¸æ»¡è¶³ â†’ é™çº§åˆ°é€šç”¨è·¯å¾„
    â”‚
    â””â”€â”€ use_ntops == False
        â””â”€â”€ ä½¿ç”¨ _infinicore.silu() (é€šç”¨ C++ è·¯å¾„)
            â”‚
            â”œâ”€â”€ inplace == True â†’ _infinicore.silu_() (åŸåœ°ä¿®æ”¹)
            â””â”€â”€ inplace == False â†’ _infinicore.silu() (æ–°å¼ é‡)
```

### 3.5 ä¸Šä¸‹æ–‡ç®¡ç†ä¸å›¾å½•åˆ¶æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              æ­£å¸¸æ‰§è¡Œæ¨¡å¼                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€ è®¾ç½®è®¾å¤‡: context.set_device(device("cuda:0"))
    â”‚   â†“ _infinicore.set_device(device._underlying)
    â”‚
    â”œâ”€â”€ æ‰§è¡Œæ“ä½œ: result = add(a, b)
    â”‚   â†“ ç«‹å³æ‰§è¡Œ C++ å†…æ ¸
    â”‚
    â””â”€â”€ åŒæ­¥: context.sync_device()
        â†“ ç­‰å¾…è®¾å¤‡å®Œæˆæ‰€æœ‰æ“ä½œ

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              å›¾å½•åˆ¶æ¨¡å¼ï¼ˆä¼˜åŒ–æ‰§è¡Œï¼‰                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€ 1. å¼€å§‹å½•åˆ¶: context.start_graph_recording()
    â”‚   â†“ _infinicore.start_graph_recording()
    â”‚
    â”œâ”€â”€ 2. è®°å½•æ“ä½œï¼ˆä¸ç«‹å³æ‰§è¡Œï¼‰
    â”‚   â”œâ”€â”€ result1 = add(a, b)        # è®°å½•åˆ°å›¾
    â”‚   â”œâ”€â”€ result2 = mul(result1, c)  # è®°å½•åˆ°å›¾
    â”‚   â””â”€â”€ result3 = rms_norm(...)    # è®°å½•åˆ°å›¾
    â”‚
    â”œâ”€â”€ 3. åœæ­¢å½•åˆ¶: graph = context.stop_graph_recording()
    â”‚   â†“ _infinicore.stop_graph_recording()
    â”‚   â†“ è¿”å› Graph å¯¹è±¡
    â”‚
    â””â”€â”€ 4. å›¾ä¼˜åŒ–ä¸é‡æ”¾
        â”œâ”€â”€ å›¾ä¼˜åŒ–: ç®—å­èåˆã€å†…å­˜åˆ†é…ä¼˜åŒ–
        â””â”€â”€ å›¾æ‰§è¡Œ: ä¸€æ¬¡æ€§æ‰§è¡Œä¼˜åŒ–åçš„å›¾
```

### 3.6 å¼ é‡ç±»å‹ç³»ç»Ÿä¸äº’æ“ä½œ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Tensor ç±»å‹å±‚æ¬¡ç»“æ„                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€ Python Tensor ç±» (tensor.py)
    â”‚   â”œâ”€â”€ _underlying: _infinicore.Tensor (åº•å±‚ C++ å¯¹è±¡)
    â”‚   â”œâ”€â”€ _torch_ref: torch.Tensor (å¯é€‰çš„ PyTorch å¼•ç”¨)
    â”‚   â”œâ”€â”€ æƒ°æ€§å±æ€§: shape, dtype, device (é¦–æ¬¡è®¿é—®æ—¶ç¼“å­˜)
    â”‚   â””â”€â”€ æ–¹æ³•: view(), permute(), squeeze(), to(), copy_()
    â”‚
    â”œâ”€â”€ InfiniCoreParameter (nn/parameter.py)
    â”‚   â””â”€â”€ Tensor çš„åŒ…è£…å™¨ï¼Œæ ‡è¯†å¯è®­ç»ƒå‚æ•°
    â”‚
    â””â”€â”€ åº•å±‚ C++ Tensor (_infinicore.Tensor)
        â”œâ”€â”€ å®é™…æ•°æ®å­˜å‚¨
        â”œâ”€â”€ å½¢çŠ¶ã€æ­¥å¹…ã€æ•°æ®ç±»å‹ä¿¡æ¯
        â””â”€â”€ è®¾å¤‡äº²å’Œæ€§

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              æ•°æ®ç±»å‹è½¬æ¢æ˜ å°„                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€ NumPy â†’ InfiniCore
    â”‚   â”œâ”€â”€ utils.numpy_to_infinicore_dtype[np.float32] â†’ dtype.float32
    â”‚   â””â”€â”€ Tensor.from_numpy(array) â†’ Tensor
    â”‚
    â”œâ”€â”€ PyTorch â†’ InfiniCore
    â”‚   â”œâ”€â”€ utils.torch_to_infinicore_dtype[torch.float32] â†’ dtype.float32
    â”‚   â””â”€â”€ Tensor.from_torch(tensor) â†’ Tensor
    â”‚
    â””â”€â”€ InfiniCore â†’ NumPy/PyTorch
        â””â”€â”€ Tensor.to(...) (æ”¯æŒè·¨æ¡†æ¶ä¼ è¾“)
```

### 3.7 å†…å­˜ä¼˜åŒ–ç­–ç•¥

**èåˆç®—å­ä¼˜åŒ–ï¼ˆadd_rms_norm æ¡ˆä¾‹ï¼‰**ï¼š

```
ä¼ ç»Ÿå®ç°ï¼ˆä¸¤æ¬¡å†…æ ¸å¯åŠ¨ï¼‰:
    â”‚
    â”œâ”€â”€ æ­¥éª¤ 1: sum = add(a, b)         # å†…æ ¸ 1: åŠ æ³•
    â”‚           â†“ ä¸­é—´ç»“æœå†™å…¥å†…å­˜
    â”‚
    â””â”€â”€ æ­¥éª¤ 2: output = rms_norm(sum)  # å†…æ ¸ 2: RMS å½’ä¸€åŒ–
                â†“ å†æ¬¡è¯»å– sum
    â”‚
    æ€»è®¡: 2 æ¬¡å†…æ ¸å¯åŠ¨, 2 æ¬¡å†…å­˜å†™å…¥, 1 æ¬¡å†…å­˜è¯»å–

èåˆç®—å­å®ç°ï¼ˆå•æ¬¡å†…æ ¸å¯åŠ¨ï¼‰:
    â”‚
    â””â”€â”€ output, residual = add_rms_norm(a, b, weight)
        â”‚
        â”œâ”€â”€ å•æ¬¡å†…æ ¸å¯åŠ¨å®Œæˆ: add + rms_norm
        â”œâ”€â”€ å‡å°‘ä¸­é—´ç»“æœå†…å­˜åˆ†é…
        â”œâ”€â”€ å‡å°‘ PCIe/å†…å­˜æ€»çº¿ä¼ è¾“
        â””â”€â”€ è¿”å›: (å½’ä¸€åŒ–ç»“æœ, æ®‹å·®è¿æ¥è¾“å…¥)
```

**In-Place æ“ä½œä¼˜åŒ–**ï¼š

```
å†…å­˜ä¼˜åŒ–çš„ FFN å‰å‘ä¼ æ’­:
    â”‚
    â”œâ”€â”€ 1. gate = linear(x, w_gate)  # æ–°å¼ é‡
    â”œâ”€â”€ 2. up = linear(x, w_up)      # æ–°å¼ é‡
    â”‚
    â”œâ”€â”€ 3. silu(gate, inplace=True)  # é‡ç”¨ gate å¼ é‡ï¼ŒåŸåœ°ä¿®æ”¹
    â”‚
    â”œâ”€â”€ 4. swiglu(gate, up, out=gate)  # ç»“æœå†™å…¥ gateï¼Œé‡ç”¨å†…å­˜
    â”‚
    â”œâ”€â”€ 5. output = linear(gate, w_down)  # æ–°å¼ é‡
    â”‚
    â””â”€â”€ 6. rms_norm(output, ..., out=output)  # åŸåœ°å½’ä¸€åŒ–

æ€»è®¡: ä»…åˆ†é… 3 ä¸ªå¼ é‡ï¼ˆgate, up, outputï¼‰ï¼Œè€Œé 6 ä¸ª
```

## 4. è®¾è®¡åŸåˆ™ä¸æœ€ä½³å®è·µ

### 4.1 PyTorch å…¼å®¹æ€§

* **API ä¸€è‡´æ€§**ï¼šå‡½æ•°ç­¾åã€å‚æ•°å‘½åã€è¿”å›å€¼ç±»å‹ä¸ PyTorch å¯¹é½ï¼ˆå¦‚ `Linear(in_features, out_features, bias=False)`ï¼‰ã€‚
* **çŠ¶æ€å­—å…¸æ ¼å¼**ï¼šä½¿ç”¨ç‚¹åˆ†éš”çš„å±‚æ¬¡åŒ–é”®åï¼ˆå¦‚ `layers.0.weight`ï¼‰ï¼Œä¸ PyTorch æ¨¡å‹äº’æ“ä½œã€‚
* **æ¨¡å—ç»„åˆæ¨¡å¼**ï¼šæ”¯æŒåµŒå¥—å­æ¨¡å—ã€å‚æ•°å…±äº«ã€ModuleList å®¹å™¨ç­‰ PyTorch æƒ¯ç”¨æ³•ã€‚
* **ç±»å‹æ³¨è§£**ï¼šä½¿ç”¨ Python ç±»å‹æ³¨è§£å¢å¼ºä»£ç å¯è¯»æ€§å’Œ IDE æ”¯æŒã€‚

### 4.2 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

* **ç®—å­èåˆ**ï¼šC++ å±‚èåˆå¤šä¸ªæ“ä½œï¼ˆå¦‚ add_rms_norm èåˆåŠ æ³•ä¸å½’ä¸€åŒ–ï¼Œcausal_softmax èåˆ softmax ä¸å› æœæ©ç ï¼‰ã€‚
* **In-Place æ“ä½œ**ï¼šæä¾› `inplace=True` å’Œ `out=` å‚æ•°æ”¯æŒå†…å­˜é‡ç”¨ï¼Œå‡å°‘å¤§æ¨¡å‹æ¨ç†çš„å†…å­˜å ç”¨ã€‚
* **åˆ†é¡µæ³¨æ„åŠ›**ï¼šé€šè¿‡å—è¡¨ï¼ˆblock tableï¼‰å®ç° KV cache çš„åŠ¨æ€åˆ†é…ï¼Œè§£å†³æ˜¾å­˜ç¢ç‰‡é—®é¢˜ï¼Œæ”¯æŒé«˜æ•ˆçš„å˜é•¿åºåˆ—æ‰¹å¤„ç†ã€‚
* **ç¡¬ä»¶åŠ é€Ÿ**ï¼šé€šè¿‡ `infinicore.use_ntops` é…ç½®ï¼Œé€‰æ‹©ç¡¬ä»¶ä¼˜åŒ–ç®—å­åº“ï¼ˆå¦‚ NVIDIA/MUSA GPU çš„ ntopsï¼‰ã€‚
* **é¢„è®¡ç®—ç­–ç•¥**ï¼šRoPE åœ¨åˆå§‹åŒ–æ—¶é¢„è®¡ç®— sin/cos æŸ¥æ‰¾è¡¨ï¼Œé¿å…å‰å‘ä¼ æ’­é‡å¤è®¡ç®—ã€‚
* **å›¾å½•åˆ¶ä¸ä¼˜åŒ–**ï¼šæ”¯æŒå›¾æ¨¡å¼å½•åˆ¶ï¼Œä¼˜åŒ–ç®—å­æ‰§è¡Œé¡ºåºå’Œå†…å­˜åˆ†é…ã€‚

### 4.3 å¤šç¡¬ä»¶åç«¯æ”¯æŒ

* **è®¾å¤‡æŠ½è±¡**ï¼šdevice ç±»æä¾›ç»Ÿä¸€çš„è®¾å¤‡æ¥å£ï¼Œæ”¯æŒ CPUã€CUDAã€MUSAã€NPU ç­‰å¤šç§è®¾å¤‡ç±»å‹ã€‚
* **è®¾å¤‡ä¸Šä¸‹æ–‡ç®¡ç†**ï¼šcontext æ¨¡å—æä¾›è®¾å¤‡åˆ‡æ¢ã€åŒæ­¥åŠŸèƒ½ï¼Œæ”¯æŒå¤šè®¾å¤‡ç¯å¢ƒä¸‹çš„è®¡ç®—ã€‚
* **è®¾å¤‡çº¦æŸæ£€æŸ¥**ï¼šéƒ¨åˆ†ç®—å­åœ¨ Python å±‚è¿›è¡Œè®¾å¤‡æ£€æŸ¥ï¼ˆå¦‚ embedding å½“å‰ä»…æ”¯æŒ CPUï¼‰ï¼Œé¿å…æ— æ•ˆçš„å†…æ ¸è°ƒç”¨ã€‚

### 4.4 æ¨¡å—åŒ–è®¾è®¡

* **èŒè´£åˆ†ç¦»**ï¼š
  * **functional å±‚**ï¼šä¸“æ³¨äºè®¡ç®—é€»è¾‘ï¼Œä¿æŒæ— çŠ¶æ€ã€å¯ç»„åˆã€å¯æµ‹è¯•ã€‚
  * **modules å±‚**ï¼šä¸“æ³¨äºçŠ¶æ€ç®¡ç†ï¼Œè´Ÿè´£å‚æ•°æ³¨å†Œã€æ¨¡å—ç»„åˆã€åºåˆ—åŒ–ã€‚
  * **ops å±‚**ï¼šæä¾›åº•å±‚ç®—å­ç»‘å®šï¼Œç›´æ¥æ˜ å°„åˆ° C++ å®ç°ã€‚
* **ä¾èµ–æ³¨å…¥**ï¼šé€šè¿‡å…¨å±€é…ç½®ï¼ˆå¦‚ `infinicore.use_ntops`ï¼‰æ§åˆ¶è¡Œä¸ºï¼Œé¿å…ç¡¬ç¼–ç ã€‚
* **è–„åŒ…è£…æ¨¡å¼**ï¼šPython å±‚ä»…åšå‚æ•°è½¬æ¢å’Œåˆ†å‘ï¼Œæ ¸å¿ƒé€»è¾‘åœ¨ C++ å±‚ï¼Œæœ€å°åŒ– Python å¼€é”€ã€‚

### 4.5 æ‰©å±•æ€§æŒ‡å—

**æ·»åŠ æ–°ç¥ç»ç½‘ç»œå±‚ï¼ˆmodules å±‚ï¼‰**ï¼š

1. ç»§æ‰¿ `InfiniCoreModule` åŸºç±»ã€‚
2. åœ¨ `__init__` ä¸­é€šè¿‡ `self.param_name = Parameter(...)` æ³¨å†Œå‚æ•°ã€‚
3. é€šè¿‡ `register_buffer()` æ³¨å†Œéå‚æ•°å¼ é‡ï¼ˆå¦‚é¢„è®¡ç®—è¡¨ï¼‰ã€‚
4. å®ç° `forward()` æ–¹æ³•ï¼Œè°ƒç”¨ `functional` å±‚çš„å‡½æ•°å®Œæˆè®¡ç®—ã€‚
5. å®ç° `extra_repr()` è¿”å›æ¨¡å—å…³é”®é…ç½®ä¿¡æ¯ã€‚
6. åœ¨ `modules/__init__.py` ä¸­å¯¼å‡ºæ–°æ¨¡å—ã€‚

**æ·»åŠ æ–°ç®—å­ï¼ˆops å±‚ï¼‰**ï¼š

1. åœ¨ C++ å±‚å®ç°ç®—å­ï¼ˆæ·»åŠ åˆ° `_infinicore` æ‰©å±•æ¨¡å—ï¼‰ã€‚
2. åœ¨ `ops/` ç›®å½•åˆ›å»ºå¯¹åº” Python æ–‡ä»¶ï¼Œç¼–å†™åŒ…è£…å‡½æ•°ã€‚
3. éµå¾ªå‘½åçº¦å®šï¼šé in-place ç‰ˆæœ¬è°ƒç”¨ `function()`ï¼Œin-place ç‰ˆæœ¬è°ƒç”¨ `function_()`ã€‚
4. æ”¯æŒå¯é€‰ `out` å‚æ•°ç”¨äºå†…å­˜ä¼˜åŒ–ã€‚
5. åœ¨ `__init__.py` ä¸­å¯¼å‡ºå‡½æ•°ã€‚

## 5. å…¸å‹åº”ç”¨åœºæ™¯

### åœºæ™¯ 1ï¼šæ„å»º Transformer è¯­è¨€æ¨¡å‹

```python
import infinicore
from infinicore.nn.modules import Module, Linear, RMSNorm, Embedding, RoPE, ModuleList
from infinicore.nn.functional import RopeAlgo

class LlamaLikeModel(Module):
    def __init__(self, vocab_size, hidden_dim, num_layers, num_heads, max_seq_len):
        super().__init__()
        self.embedding = Embedding(vocab_size, hidden_dim)
        self.layers = ModuleList([
            TransformerBlock(hidden_dim, num_heads, max_seq_len)
            for _ in range(num_layers)
        ])
        self.norm = RMSNorm(hidden_dim)

    def forward(self, input_ids, position_ids):
        # 1. è¯åµŒå…¥
        hidden = self.embedding(input_ids)

        # 2. å †å  Transformer å±‚
        for layer in self.layers:
            hidden = layer(hidden, position_ids)

        # 3. æœ€ç»ˆå½’ä¸€åŒ–
        hidden = self.norm(hidden)

        return hidden

# ä¿å­˜/åŠ è½½æ¨¡å‹æƒé‡
state_dict = model.state_dict()
model.load_state_dict(state_dict)
```

### åœºæ™¯ 2ï¼šä½¿ç”¨åˆ†é¡µæ³¨æ„åŠ›çš„é«˜æ•ˆæ‰¹é‡æ¨ç†

```python
from infinicore.ops import paged_attention_prefill, paged_attention, paged_caching

def batch_inference_with_paged_attention(
    model, prompts_batch, k_cache, v_cache, block_tables
):
    """ä½¿ç”¨åˆ†é¡µæ³¨æ„åŠ›è¿›è¡Œæ‰¹é‡æ¨ç†ï¼Œæ”¯æŒå˜é•¿åºåˆ—"""

    # 1. é¢„å¡«å……é˜¶æ®µï¼šå¤„ç†æ‰€æœ‰ prompt
    prefill_output = paged_attention_prefill(
        q=prompts_q,
        k_cache=k_cache,
        v_cache=v_cache,
        block_tables=block_tables,
        history_lens=history_lens,
        cu_seqlens_q=cu_seqlens_q,
        scale=1.0 / sqrt(head_dim)
    )

    # 2. è§£ç é˜¶æ®µï¼šè‡ªå›å½’ç”Ÿæˆ
    for step in range(max_steps):
        # è®¡ç®— new token çš„ QKV
        q, k, v = model.forward(last_token)

        # å†™å…¥ KV cache
        slot_mapping = compute_slot_mapping(...)
        paged_caching(k_cache, v_cache, k, v, slot_mapping)

        # è®¡ç®—æ³¨æ„åŠ›ï¼ˆä¸å†å² cacheï¼‰
        token_output = paged_attention(
            q, k_cache, v_cache, block_tables, cache_lens,
            scale=1.0 / sqrt(head_dim)
        )

        last_token = token_output

    return output
```

### åœºæ™¯ 3ï¼šä½¿ç”¨èåˆç®—å­ä¼˜åŒ–å†…å­˜

```python
from infinicore.ops import add_rms_norm

def transformer_layer_with_fusion(hidden, input, weight):
    """ä½¿ç”¨èåˆç®—å­ä¼˜åŒ– Transformer å±‚"""

    # ä¼ ç»Ÿå®ç°ï¼ˆä¸¤æ¬¡å†…æ ¸è°ƒç”¨ï¼‰:
    # residual = add(hidden, input)
    # normalized = rms_norm(residual, weight)

    # èåˆå®ç°ï¼ˆå•æ¬¡å†…æ ¸è°ƒç”¨ï¼‰:
    normalized, residual = add_rms_norm(
        a=hidden,
        b=input,
        weight=weight,
        epsilon=1e-5
    )

    # residual å¯ç›´æ¥ç”¨äºä¸‹ä¸€å±‚çš„æ®‹å·®è¿æ¥
    output = attention(normalized) + mlp(normalized) + residual

    return output
```

## 6. ä¾èµ–å…³ç³»å›¾

```
infinicore (æ ¹åŒ…)
    â”‚
    â”œâ”€â”€ å†…éƒ¨ä¾èµ–
    â”‚   â”œâ”€â”€ infinicore.lib._infinicore (C++ æ‰©å±•æ¨¡å—ï¼Œæ ¸å¿ƒç»‘å®š)
    â”‚   â”œâ”€â”€ infinicore.nn (ç¥ç»ç½‘ç»œå­æ¨¡å—)
    â”‚   â”‚   â”œâ”€â”€ infinicore.nn.functional (å‡½æ•°å¼ API)
    â”‚   â”‚   â”œâ”€â”€ infinicore.nn.modules (æ¨¡å—å°è£…)
    â”‚   â”‚   â””â”€â”€ infinicore.nn.parameter (å‚æ•°ç±»å‹)
    â”‚   â”œâ”€â”€ infinicore.ops (ç®—å­ç»‘å®š)
    â”‚   â”œâ”€â”€ infinicore.tensor (Tensor ç±»)
    â”‚   â”œâ”€â”€ infinicore.device (device ç±»)
    â”‚   â”œâ”€â”€ infinicore.dtype (dtype ç±»)
    â”‚   â”œâ”€â”€ infinicore.context (ä¸Šä¸‹æ–‡ç®¡ç†)
    â”‚   â”œâ”€â”€ infinicore.device_event (äº‹ä»¶åŒæ­¥)
    â”‚   â”œâ”€â”€ infinicore.graph (è®¡ç®—å›¾)
    â”‚   â””â”€â”€ infinicore.utils (å·¥å…·å‡½æ•°)
    â”‚
    â”œâ”€â”€ Python æ ‡å‡†åº“
    â”‚   â”œâ”€â”€ collections.OrderedDict
    â”‚   â”œâ”€â”€ typing (ç±»å‹æ³¨è§£)
    â”‚   â”œâ”€â”€ itertools.chain
    â”‚   â”œâ”€â”€ numbers.Integral
    â”‚   â”œâ”€â”€ contextlib
    â”‚   â””â”€â”€ ctypes
    â”‚
    â””â”€â”€ å¤–éƒ¨ä¾èµ–ï¼ˆæ¡ä»¶ä¾èµ–ï¼‰
        â”œâ”€â”€ numpy (æ•°æ®ç±»å‹è½¬æ¢ã€å¼ é‡äº’æ“ä½œ)
        â”œâ”€â”€ torch (å¯é€‰ï¼ŒPyTorch å¼ é‡äº’æ“ä½œ)
        â””â”€â”€ ntops (å¯é€‰ï¼Œç¡¬ä»¶åŠ é€Ÿåº“ï¼Œç”¨äº CUDA/MUSA GPU)
```

## 7. æ€§èƒ½ç‰¹å¾

* **è®¡ç®—å¤æ‚åº¦**ï¼š
  * Linear: O(batch_size * in_features * out_features)
  * causal_softmax: O(batch_size * num_heads * seq_len^2) - æ³¨æ„åŠ›ç“¶é¢ˆ
  * rms_norm: O(batch_size * seq_len * hidden_dim)
  * rope: O(batch_size * seq_len * num_heads * head_dim) - æŸ¥æ‰¾è¡¨æ“ä½œ
  * paged_attention: O(batch_size * num_heads * seq_len * cache_len) - ä¼˜åŒ–çš„æ³¨æ„åŠ›è®¡ç®—

* **å†…å­˜å ç”¨**ï¼š
  * æ¨¡å—å‚æ•°ï¼šO(total_parameters) - ç”±æ¨¡å‹å¤§å°å†³å®š
  * RoPE æŸ¥æ‰¾è¡¨ï¼šO(max_position_embeddings * head_dim) - å›ºå®šå¼€é”€
  * åˆ†é¡µ KV Cacheï¼šåŠ¨æ€åˆ†é…ï¼ŒåŸºäºå—è¡¨ç®¡ç†ï¼Œå‡å°‘å†…å­˜ç¢ç‰‡
  * å‰å‘ä¼ æ’­ä¸­é—´ç»“æœï¼šé€šè¿‡ in-place æ“ä½œå’Œèåˆç®—å­ä¼˜åŒ–å†…å­˜å ç”¨

* **ä¼˜åŒ–çº§åˆ«**ï¼š
  * C++ å†…æ ¸ï¼šä½¿ç”¨ SIMDã€å¹¶è¡Œç®—æ³•ã€ç®—å­èåˆ
  * ç¡¬ä»¶åŠ é€Ÿï¼šé’ˆå¯¹ CUDA/MUSA çš„ä¼˜åŒ–å†…æ ¸ï¼ˆntopsï¼‰
  * åˆ†é¡µæ³¨æ„åŠ›ï¼šé«˜æ•ˆçš„ KV cache ç®¡ç†ï¼Œæ”¯æŒå˜é•¿åºåˆ—æ‰¹å¤„ç†
  * Python å±‚ï¼šæœ€å°åŒ–å¼€é”€ï¼Œç›´æ¥è½¬å‘åˆ° C++ å±‚
  * å›¾ä¼˜åŒ–ï¼šæ”¯æŒå›¾å½•åˆ¶æ¨¡å¼ï¼Œä¼˜åŒ–ç®—å­æ‰§è¡Œé¡ºåº

## 8. ä¸ InfiniCore æ•´ä½“æ¶æ„çš„å…³ç³»

```
InfiniCore æ•´ä½“æ¶æ„
    â”‚
    â”œâ”€â”€ ä¸Šå±‚åº”ç”¨
    â”‚   â”œâ”€â”€ InfiniLM (å¤§è¯­è¨€æ¨¡å‹)
    â”‚   â”œâ”€â”€ InfiniTrain (è®­ç»ƒæ¡†æ¶)
    â”‚   â””â”€â”€ infiniStudio (å¯è§†åŒ–å·¥å…·)
    â”‚
    â”œâ”€â”€ Python å‰ç«¯å±‚ (å½“å‰ç›®å½•)
    â”‚   â”œâ”€â”€ infinicore.nn (ç¥ç»ç½‘ç»œ API)
    â”‚   â”œâ”€â”€ infinicore.ops (ç®—å­ç»‘å®š)
    â”‚   â””â”€â”€ infinicore (æ ¸å¿ƒç±»å‹)
    â”‚
    â”œâ”€â”€ C++ API å±‚
    â”‚   â”œâ”€â”€ Tensor C++ API
    â”‚   â”œâ”€â”€ Operator C++ API
    â”‚   â””â”€â”€ Runtime C++ API
    â”‚
    â””â”€â”€ ç¡¬ä»¶åç«¯å±‚
        â”œâ”€â”€ CPU åç«¯
        â”œâ”€â”€ CUDA åç«¯
        â”œâ”€â”€ MUSA åç«¯
        â”œâ”€â”€ Kunlun åç«¯
        â”œâ”€â”€ Ascend åç«¯
        â””â”€â”€ å…¶ä»–ç¡¬ä»¶åç«¯
```

**æœ¬ç›®å½•çš„è§’è‰²**ï¼šPython å‰ç«¯å±‚ï¼Œæä¾›ç”¨æˆ·å‹å¥½çš„ Python APIï¼Œé€šè¿‡ pybind11 ç»‘å®šåˆ° C++ å®ç°ï¼Œæ˜¯ InfiniCore æ¡†æ¶ä¸ç”¨æˆ·äº¤äº’çš„å…¥å£ç‚¹ã€‚
