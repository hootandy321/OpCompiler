//   输入的两个维度是 (tokens, dim)，具体含义：

//   第一维 tokens：是 batch_size × seq_len 展平后的结果。LLM 推理中，hidden state 原始形状为 (batch, seq_len,
//   hidden_dim)，在送入逐元素算子前通常 reshape 成 2D (batch * seq_len, hidden_dim)。所以 512 可以理解为 batch=1, seq=512，或
//   batch=8, seq=64 等。

//   第二维 dim：语义取决于算子所在的网络位置：
//   ┌──────────────┬─────────────────────┬──────────────────────────────────┬─────────────────────────────────────┐
//   │     类别     │      对应算子       │             dim 含义             │               典型值                │
//   ├──────────────┼─────────────────────┼──────────────────────────────────┼─────────────────────────────────────┤
//   │ HIDDEN_TIERS │ add, rms_norm, gelu │ hidden_dim，模型隐藏层宽度       │ 4096 (LLaMA-7B), 8192 (LLaMA-70B)   │
//   ├──────────────┼─────────────────────┼──────────────────────────────────┼─────────────────────────────────────┤
//   │ INTER_TIERS  │ silu, mul           │ intermediate_dim，FFN 中间层宽度 │ 11008 (LLaMA-7B), 14336 (LLaMA-70B) │
//   └──────────────┴─────────────────────┴──────────────────────────────────┴─────────────────────────────────────┘
//   区分两组 tier 的原因是 LLM 的 FFN 结构：SwiGLU 的 gate/up 投影输出维度是 intermediate_dim（通常 ≈ 2.7×
//   hidden_dim），而残差连接、RMSNorm、GELU 作用在 hidden_dim 上。

//   三档负载对应的实际场景参考：
//   ┌──────┬────────┬─────────────────────────────┐
//   │ 档次 │ tokens │            场景             │
//   ├──────┼────────┼─────────────────────────────┤
//   │ low  │ 512    │ 单条短序列推理              │
//   ├──────┼────────┼─────────────────────────────┤
//   │ mid  │ 16384  │ 中等 batch 或长序列         │
//   ├──────┼────────┼─────────────────────────────┤
//   │ high │ 65536  │ 大 batch × 长序列，压力测试 │
//   └──────┴────────┴─────────────────────────────┘

{
  "single": {
    "silu": {
      "[512, 11008] (low)": "0.0432 ms",
      "[16384, 11008] (mid)": "0.5615 ms",
      "[65536, 14336] (high)": "2.8925 ms"
    },
    "gelu": {
      "[512, 4096] (low)": "0.0168 ms",
      "[16384, 4096] (mid)": "0.3002 ms",
      "[65536, 8192] (high)": "2.8095 ms"
    },
    "add": {
      "[512, 4096] (low)": "0.0134 ms",
      "[16384, 4096] (mid)": "0.2703 ms",
      "[65536, 8192] (high)": "2.1173 ms"
    },
    "mul": {
      "[512, 11008] (low)": "0.0287 ms",
      "[16384, 11008] (mid)": "0.7202 ms",
      "[65536, 14336] (high)": "3.7009 ms"
    },
    "rms_norm": {
      "[512, 4096] (low)": "0.1103 ms",
      "[16384, 4096] (mid)": "1.6723 ms",
      "[65536, 8192] (high)": "12.9401 ms"
    }
  },
  "fused": {
    "add+rms_norm": {
      "[512, 4096] (low)": {
        "unfused_ms": "0.1240",
        "fused_ms": "125.6215",
        "speedup": "0.00x"
      },
      "[16384, 4096] (mid)": {
        "unfused_ms": "1.9518",
        "fused_ms": "126.5437",
        "speedup": "0.02x"
      },
      "[65536, 8192] (high)": {
        "unfused_ms": "15.0630",
        "fused_ms": "130.1603",
        "speedup": "0.12x"
      }
    },
    "silu+mul": {
      "[512, 11008] (low)": {
        "unfused_ms": "0.0527",
        "fused_ms": "0.1043",
        "speedup": "0.51x"
      },
      "[16384, 11008] (mid)": {
        "unfused_ms": "1.2816",
        "fused_ms": "1.2817",
        "speedup": "1.00x"
      }
    },
    "gelu": {
      "[512, 4096] (low)": {
        "unfused_ms": "0.0177",
        "fused_ms": "0.0175",
        "speedup": "1.01x"
      },
      "[16384, 4096] (mid)": {
        "unfused_ms": "0.3009",
        "fused_ms": "0.3008",
        "speedup": "1.00x"
      },
      "[65536, 8192] (high)": {
        "unfused_ms": "2.7979",
        "fused_ms": "2.7972",
        "speedup": "1.00x"
      }
    }
  }
}